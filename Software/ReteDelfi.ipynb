{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1) Importazione librerie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import scipy as sci\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input\n",
    "from tensorflow.keras import saving\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score, confusion_matrix, mean_squared_error\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "import gc\n",
    "\n",
    "import ImageFilter as filter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T16:56:09.590798Z",
     "start_time": "2023-09-10T16:56:09.579799400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2) Caricamento dataset\n",
    "Le variabili _whistleImagesDirectory, noiseImagesDirectory_ contengono rispettivamente l'indirizzo della locazione in memoria dei file appartenenti alla classe dei delfini (1) e la classe dei rumori(0).\n",
    "\n",
    "Prima vengono caricate le immagini dei fischi, successivamente quelle dei rumori. In entrambi i casi, ogni file viene caricato e ridimensionato nelle dimensioni (224,224,1), quindi sono immagini 2D.\n",
    "\n",
    "I file di entrambe le classi vengono aggiunte alla lista _images_, mentre alla lista _labels_ vengono aggiunte le etichette in formato numerico. Per le immagini che rappresentano un delfino l'etichetta è 1, mentre i rumori hanno l'etichetta 0.\n",
    "\n",
    "Poi, tutte le immagini vengono normalizzate, quindi i valori dei pixel non hanno più un valore tra 0 e 255, ma tra 0 e 1.\n",
    "\n",
    "Successivamente, è possibile suddividere il dataset in train, validation (opzionale) e test nelle percentuali desiderate.\n",
    "\n",
    "Al momento dell'addestramento della rete neurale (più avanti), si può scegliere di utilizzare la Fold Cross-Validation, in cui viene effettuata una suddivisione del dataset in train e test differente per ogni fold. Per cui, in tal caso non è necessario eseguire la suddivisione del dataset in questo blocco di codice perchè viene effettuata dopo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set shapes: (1367, 224, 224) (1367,)\n",
      "Test set shapes: (152, 224, 224) (152,)\n"
     ]
    }
   ],
   "source": [
    "# Directory containing images\n",
    "whistleImagesDirectory = 'Data\\\\Sobel\\\\Whistle'\n",
    "noiseImagesDirectory = 'Data\\\\Sobel\\\\Noise'\n",
    "\n",
    "# List all image files in the directory\n",
    "whistleFiles = os.listdir(whistleImagesDirectory)\n",
    "noiseFiles = os.listdir(noiseImagesDirectory)\n",
    "\n",
    "# Initialize empty lists for images and labels\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "# Load and preprocess whistle images\n",
    "for image_file in whistleFiles:\n",
    "    image_path = os.path.join(whistleImagesDirectory, image_file)\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = image.resize((224, 224))  # Resize\n",
    "    image = np.array(image)  # Convert to NumPy array\n",
    "    images.append(image)\n",
    "    labels.append(1)    #etichetta delfino\n",
    "\n",
    "\n",
    "# Load and preprocess noise images\n",
    "for image_file in noiseFiles:\n",
    "    image_path = os.path.join(noiseImagesDirectory, image_file)\n",
    "    image = Image.open(image_path).convert('L')  # Convert to grayscale\n",
    "    image = image.resize((224, 224))\n",
    "    image = np.array(image)\n",
    "    images.append(image)\n",
    "    labels.append(0)    #etichetta rumore\n",
    "\n",
    "\n",
    "X = np.array(images)/255.0\n",
    "y = np.array(labels)\n",
    "\n",
    "# Split the dataset into training and test sets                 (90/10)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_val, y_val = [], []\n",
    "\n",
    "# Split the dataset into training, validation, and test sets    (70/20/10)\n",
    "#X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "#X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.33, random_state=42)\n",
    "\n",
    "# Print the shapes of the resulting sets\n",
    "print(\"Train set shapes:\", X_train.shape, y_train.shape)\n",
    "#print(\"Validation set shapes:\", X_val.shape, y_val.shape)\n",
    "print(\"Test set shapes:\", X_test.shape, y_test.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T20:58:42.273929900Z",
     "start_time": "2023-09-10T20:58:36.037604100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "1.0"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verifica normalizzazione, il valore deve essere contenuto tra 0 e 1.\n",
    "print(np.max(X))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T15:13:58.734734800Z",
     "start_time": "2023-09-09T15:13:58.678539200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Visualizzazione informazione dataset, riguardo al numero di fischi e di rumori presenti nel dataset intero e in quelli di train, validation e test."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Whistle in total dataset: 596\n",
      "Noise in total dataset: 923\n",
      "\n",
      "Whistle in dataset train: 530\n",
      "Noise in dataset train: 837\n",
      "\n",
      "Whistle in dataset validation: 0\n",
      "Noise in dataset validation: 0\n",
      "\n",
      "Whistle in dataset test: 66\n",
      "Noise in dataset test: 86\n",
      "\n"
     ]
    }
   ],
   "source": [
    "totW = 0\n",
    "totN = 0\n",
    "\n",
    "def printAndSaveData(nameDivision, whistle, noise):\n",
    "    print(f'Whistle in {nameDivision}: {whistle}')\n",
    "    print(f'Noise in {nameDivision}: {noise}\\n')\n",
    "\n",
    "\n",
    "def getStatsDataset(y_true):\n",
    "    nWhistle = 0\n",
    "    nNoise = 0\n",
    "    y_true = np.array(y_true)\n",
    "    nWhistle += np.count_nonzero(y_true == 1)\n",
    "    nNoise += np.count_nonzero(y_true == 0)\n",
    "    return nWhistle, nNoise\n",
    "\n",
    "\n",
    "nWhistle, nNoise = getStatsDataset(y)\n",
    "printAndSaveData('total dataset', nWhistle, nNoise)\n",
    "\n",
    "nWhistle, nNoise = getStatsDataset(y_train)\n",
    "totW += nWhistle\n",
    "totN += nNoise\n",
    "printAndSaveData('dataset train', nWhistle, nNoise)\n",
    "\n",
    "nWhistle, nNoise = getStatsDataset(y_val)\n",
    "totW += nWhistle\n",
    "totN += nNoise\n",
    "printAndSaveData('dataset validation', nWhistle, nNoise)\n",
    "\n",
    "nWhistle, nNoise = getStatsDataset(y_test)\n",
    "totW += nWhistle\n",
    "totN += nNoise\n",
    "printAndSaveData('dataset test', nWhistle, nNoise)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T15:45:23.534715600Z",
     "start_time": "2023-09-09T15:45:23.518723100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 3) Architettura modello"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "def getModel():\n",
    "    model = tf.keras.models.Sequential([\n",
    "\n",
    "    #Input Layer\n",
    "    Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "\n",
    "    #Secondo Layer\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    #Terzo Layer\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    #Quarto Layer\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    #Quinto Layer\n",
    "    Conv2D(64, (3,3), activation='relu'),\n",
    "    MaxPooling2D(2,2),\n",
    "\n",
    "    Flatten(),\n",
    "\n",
    "    #Primo Dense Layer\n",
    "    Dense(512, activation='relu'),\n",
    "\n",
    "    #Output Dense Layer\n",
    "    Dense(1, activation='sigmoid')])\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T15:47:36.203610700Z",
     "start_time": "2023-09-09T15:47:36.181610700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_14 (Conv2D)          (None, 222, 222, 32)      320       \n",
      "                                                                 \n",
      " max_pooling2d_14 (MaxPooli  (None, 111, 111, 32)      0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_15 (Conv2D)          (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_15 (MaxPooli  (None, 54, 54, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_16 (Conv2D)          (None, 52, 52, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_16 (MaxPooli  (None, 26, 26, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_17 (Conv2D)          (None, 24, 24, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPooli  (None, 12, 12, 64)        0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " conv2d_18 (Conv2D)          (None, 10, 10, 64)        36928     \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPooli  (None, 5, 5, 64)          0         \n",
      " ng2D)                                                           \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 1600)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 512)               819712    \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 513       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 949825 (3.62 MB)\n",
      "Trainable params: 949825 (3.62 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#Visualizzazione numero di parametri del modello\n",
    "model = getModel()\n",
    "model.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T14:12:15.166194Z",
     "start_time": "2023-09-10T14:12:14.968948900Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 4) Addestramento modello\n",
    "Esistono diversi tipi di addestramento. Vengono riportati i principali, ovvero quello normale e quello con la Fold Cross-Validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.1) Addestramento senza Fold Cross-Validation\n",
    "Utilizzo ottimizzatore \"Adam\" con learning rate 0.0001. Con un learning rate più piccolo, la rete ha bisogno di più epoche per imparare. La funzione di loss impostata è \"BinaryCrossentropy\", ideale per problemi di classificazione binaria. L'accuracy è la metrica scelta.\n",
    "\n",
    "Per addestrare il modello, bisogna scegliere un numero di epoche. Poche epoche non permettono alla rete di imparare bene, troppe epoche possono invece causare l'overfitting. Per cui è stato utilizzato l'EarlyStopping che lascia addestrare la modello fino ad un numero massimo di epoche settato a 100, monitorando in questo caso il valore di \"val_loss\" con pazienza 15. Se dopo 15 epoche di fila il valore di \"val_loss\" non migliora, l'addestramento viene interrotto.\n",
    "\n",
    "Viene inoltre settato il batch size a 32.\n",
    "\n",
    "L'attributo class_weights viene utilizzato per fare in modo che le due classi siano \"sbilanciate\". Con questi valori, la classe dei rumori ha più peso rispetto alla classe dei delfini. Ciò è stato fatto in quanto si preferisce avere dei falsi negativi piuttosto che dei falsi positivi.\n",
    "\n",
    "Utilizzare anche l'attributo \"validation_data\" solo se parte del dataset è stato suddiviso anche per la validazione.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer, loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "class_weights = {0: 1.5,  # noise\n",
    "                 1: 1.0}  # dolphin's whistle\n",
    "\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=15, verbose=1)\n",
    "hist = model.fit(X_train, y_train, batch_size=32, epochs=100, class_weight=class_weights, callbacks=[early_stop])\n",
    "#hist = model.fit(X_train, y_train, batch_size=32, epochs=100, validation_data=(X_val, y_val),\n",
    "#                 class_weight=class_weights, callbacks=[early_stop])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4.2) Fold Cross-Validation\n",
    "In questa parte di codice, viene effettuata una 5 Fold Cross-Validation."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.2.1) Addestramento modelli\n",
    "Il dataset intero viene suddiviso in 5 parti (20% ciascuna). Vengono addestrati 5 modelli della stessa architettura, ma con un dataset differente. Pre ogni modello viene utilizzata una delle 5 fold differente come dataset di test, le altre invece per il train. L'addestramento per tutti i modelli è lo stesso riportato al blocco di codice 4.1). Dopo l'addestramento, con una soglia di 0.5 vengono calcolati Precision, Recall, Accuracy, F1-Score e MSE. Inoltre, viene calcolata anche la Confusion Matrix che riporta il numero di TP,TN,FP,FN.\n",
    "Ricapitolando, tutti i modelli sono stati addestrati con il dataset suddiviso in 80% train e 20% test, ma il materiale utilizzato è diverso per ogni modello."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on fold 1/5\n",
      "1215 304\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 40s 1s/step - loss: 0.8177 - accuracy: 0.6082\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 36s 950ms/step - loss: 0.8027 - accuracy: 0.6082\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.7990 - accuracy: 0.6082\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 0.7998 - accuracy: 0.6082\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 0.8017 - accuracy: 0.6082\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.8031 - accuracy: 0.6082\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.7981 - accuracy: 0.6082\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 0.7933 - accuracy: 0.6082\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.7919 - accuracy: 0.6082\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 33s 879ms/step - loss: 0.7309 - accuracy: 0.6263\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 0.4377 - accuracy: 0.8370\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.2284 - accuracy: 0.9350\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 0.2015 - accuracy: 0.9366\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 0.1412 - accuracy: 0.9572\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 0.1141 - accuracy: 0.9663\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.1085 - accuracy: 0.9737\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.1164 - accuracy: 0.9737\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 32s 835ms/step - loss: 0.0852 - accuracy: 0.9770\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 0.0772 - accuracy: 0.9819\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 35s 931ms/step - loss: 0.0710 - accuracy: 0.9819\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 35s 935ms/step - loss: 0.0636 - accuracy: 0.9852\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 0.0607 - accuracy: 0.9852\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 0.0563 - accuracy: 0.9852\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 39s 1s/step - loss: 0.0508 - accuracy: 0.9885\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 0.0492 - accuracy: 0.9901\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 36s 960ms/step - loss: 0.0456 - accuracy: 0.9901\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 35s 934ms/step - loss: 0.0466 - accuracy: 0.9885\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 0.0383 - accuracy: 0.9934\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.0421 - accuracy: 0.9877\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 32s 852ms/step - loss: 0.0347 - accuracy: 0.9926\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 36s 935ms/step - loss: 0.0315 - accuracy: 0.9926\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 0.0335 - accuracy: 0.9901\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 0.0377 - accuracy: 0.9893\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 34s 895ms/step - loss: 0.0344 - accuracy: 0.9918\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 0.0318 - accuracy: 0.9918\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 0.0247 - accuracy: 0.9942\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 34s 881ms/step - loss: 0.0184 - accuracy: 0.9951\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 32s 847ms/step - loss: 0.0259 - accuracy: 0.9926\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 0.0160 - accuracy: 0.9967\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.0160 - accuracy: 0.9967\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.0129 - accuracy: 0.9967\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 32s 829ms/step - loss: 0.0142 - accuracy: 0.9959\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.0184 - accuracy: 0.9951\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 32s 843ms/step - loss: 0.0191 - accuracy: 0.9951\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.0255 - accuracy: 0.9942\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0088 - accuracy: 0.9975\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0086 - accuracy: 0.9975\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0138 - accuracy: 0.9951\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 0.0527 - accuracy: 0.9835\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0253 - accuracy: 0.9926\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.0117 - accuracy: 0.9967\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 31s 804ms/step - loss: 0.0096 - accuracy: 0.9984\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 0.0061 - accuracy: 0.9992\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 0.0046 - accuracy: 0.9992\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.0060 - accuracy: 0.9984\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.0049 - accuracy: 0.9992\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 31s 804ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.0021 - accuracy: 0.9992\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 0.0038 - accuracy: 0.9992\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.0062 - accuracy: 0.9984\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0031 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0018 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 32s 829ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 32s 841ms/step - loss: 8.7031e-04 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 6.7247e-04 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 6.5135e-04 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 7.8545e-04 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 5.5370e-04 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 5.0865e-04 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 5.2556e-04 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 35s 924ms/step - loss: 4.7255e-04 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 34s 898ms/step - loss: 4.9393e-04 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 34s 905ms/step - loss: 4.4694e-04 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 35s 917ms/step - loss: 4.2855e-04 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 35s 919ms/step - loss: 4.1214e-04 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 34s 907ms/step - loss: 3.8062e-04 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 5.0740e-04 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 33s 880ms/step - loss: 4.4090e-04 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 3.1487e-04 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 35s 916ms/step - loss: 2.7557e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 35s 911ms/step - loss: 3.1888e-04 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 37s 969ms/step - loss: 3.2101e-04 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 35s 933ms/step - loss: 4.1857e-04 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 33s 874ms/step - loss: 2.6666e-04 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 34s 888ms/step - loss: 2.4777e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 35s 913ms/step - loss: 2.9245e-04 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 35s 921ms/step - loss: 2.3912e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 34s 882ms/step - loss: 2.4825e-04 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 2.1716e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 34s 884ms/step - loss: 2.1747e-04 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 34s 891ms/step - loss: 2.1241e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 2.0045e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 1.7713e-04 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 1.7758e-04 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 33s 879ms/step - loss: 1.7097e-04 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 33s 881ms/step - loss: 1.8814e-04 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 34s 893ms/step - loss: 1.7315e-04 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 35s 913ms/step - loss: 1.6763e-04 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 34s 905ms/step - loss: 1.5802e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 34s 908ms/step - loss: 1.5288e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 33s 858ms/step - loss: 1.4654e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 1.5941e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 1.7811e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 1.4728e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 1.2389e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 1.4481e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 1.1863e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 1.0766e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 1.0528e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 1.0960e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 1.0595e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 9.5277e-05 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 9.3917e-05 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 1.0690e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 31s 818ms/step - loss: 9.8548e-05 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 8.9921e-05 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 7.7598e-05 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 7.6643e-05 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 1.1319e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 31s 804ms/step - loss: 8.4529e-05 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 1.0209e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 9.1893e-05 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 6.5727e-05 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 6.7710e-05 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 6.0183e-05 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 5.9652e-05 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 5.9110e-05 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 5.4618e-05 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 5.5510e-05 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 5.6050e-05 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 5.0154e-05 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 31s 816ms/step - loss: 5.6695e-05 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 4.8820e-05 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 4.8053e-05 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 32s 843ms/step - loss: 4.1078e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 32s 843ms/step - loss: 5.2219e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 4.8419e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 5.1045e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 4.0745e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 5.1127e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 31s 818ms/step - loss: 4.3628e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 3.7123e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 3.6017e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 3.6055e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 3.9210e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 31s 808ms/step - loss: 3.8406e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 3.4423e-05 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 3.2103e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 3.0413e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 3.3175e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 2.9725e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 2.7395e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 2.5878e-05 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 2.5070e-05 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 2.6125e-05 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 2.4878e-05 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 34s 899ms/step - loss: 2.1465e-05 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 37s 977ms/step - loss: 2.4938e-05 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 35s 922ms/step - loss: 2.1540e-05 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 34s 906ms/step - loss: 2.1924e-05 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 36s 940ms/step - loss: 2.0735e-05 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 36s 954ms/step - loss: 2.1900e-05 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 36s 938ms/step - loss: 2.5413e-05 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 34s 887ms/step - loss: 2.0301e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 35s 914ms/step - loss: 1.8337e-05 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 34s 895ms/step - loss: 1.7711e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 33s 882ms/step - loss: 1.8771e-05 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 34s 886ms/step - loss: 1.6709e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 35s 913ms/step - loss: 1.6657e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 1.7838e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 1.7484e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 34s 883ms/step - loss: 1.6826e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 34s 898ms/step - loss: 1.4634e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 34s 898ms/step - loss: 1.4019e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 1.4647e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 1.4439e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 34s 886ms/step - loss: 1.3264e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 33s 880ms/step - loss: 1.3275e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 1.2310e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 34s 891ms/step - loss: 1.1986e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 34s 882ms/step - loss: 1.2497e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 1.1225e-05 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 1.2723e-05 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 34s 884ms/step - loss: 1.1993e-05 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 35s 912ms/step - loss: 1.0985e-05 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 34s 883ms/step - loss: 1.0482e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 1.0903e-05 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 33s 874ms/step - loss: 1.0119e-05 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "38/38 [==============================] - 35s 912ms/step - loss: 9.2923e-06 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 9.8050e-06 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 9.4578e-06 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 1.1035e-05 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 1.0312e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 8.2626e-06 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 7.8631e-06 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 32s 845ms/step - loss: 7.7665e-06 - accuracy: 1.0000\n",
      "10/10 [==============================] - 2s 196ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision:  1.0\n",
      "Recall:  0.975\n",
      "Accuracy:  0.9901315789473685\n",
      "F1-Score:  0.9873417721518987\n",
      "\n",
      "True Positives (TP): 117\n",
      "True Negatives (TN): 184\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 3\n",
      "\n",
      "Mean Squared Error: 0.009868421052631578\n",
      "Training on fold 2/5\n",
      "1215 304\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 34s 877ms/step - loss: 0.8180 - accuracy: 0.6066\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.8003 - accuracy: 0.6066\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 32s 845ms/step - loss: 0.8007 - accuracy: 0.6066\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 36s 945ms/step - loss: 0.7981 - accuracy: 0.6066\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 36s 940ms/step - loss: 0.7999 - accuracy: 0.6066\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 35s 929ms/step - loss: 0.8023 - accuracy: 0.6066\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 37s 983ms/step - loss: 0.7984 - accuracy: 0.6066\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 38s 1s/step - loss: 0.7997 - accuracy: 0.6066\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 42s 1s/step - loss: 0.7897 - accuracy: 0.6066\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 38s 991ms/step - loss: 0.7426 - accuracy: 0.6156\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 37s 969ms/step - loss: 0.4888 - accuracy: 0.8239\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 37s 973ms/step - loss: 0.2630 - accuracy: 0.9210\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 37s 965ms/step - loss: 0.1804 - accuracy: 0.9481\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 37s 975ms/step - loss: 0.1470 - accuracy: 0.9621\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 37s 964ms/step - loss: 0.1298 - accuracy: 0.9638\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 37s 967ms/step - loss: 0.1208 - accuracy: 0.9630\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 37s 969ms/step - loss: 0.0996 - accuracy: 0.9753\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 37s 988ms/step - loss: 0.0887 - accuracy: 0.9786\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 38s 996ms/step - loss: 0.0917 - accuracy: 0.9786\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 37s 970ms/step - loss: 0.0784 - accuracy: 0.9778\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 37s 970ms/step - loss: 0.0716 - accuracy: 0.9819\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 34s 895ms/step - loss: 0.0707 - accuracy: 0.9844\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 34s 884ms/step - loss: 0.0589 - accuracy: 0.9827\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 34s 883ms/step - loss: 0.0537 - accuracy: 0.9877\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 34s 893ms/step - loss: 0.0574 - accuracy: 0.9835\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 35s 911ms/step - loss: 0.0567 - accuracy: 0.9844\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 34s 905ms/step - loss: 0.0439 - accuracy: 0.9893\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 34s 891ms/step - loss: 0.0413 - accuracy: 0.9893\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 34s 890ms/step - loss: 0.0386 - accuracy: 0.9877\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 0.0327 - accuracy: 0.9877\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 34s 907ms/step - loss: 0.0303 - accuracy: 0.9926\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 35s 913ms/step - loss: 0.0459 - accuracy: 0.9885\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 34s 887ms/step - loss: 0.0298 - accuracy: 0.9909\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.0256 - accuracy: 0.9918\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 0.0309 - accuracy: 0.9918\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 34s 896ms/step - loss: 0.0299 - accuracy: 0.9901\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 34s 890ms/step - loss: 0.0343 - accuracy: 0.9893\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 34s 887ms/step - loss: 0.0206 - accuracy: 0.9951\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 35s 909ms/step - loss: 0.0204 - accuracy: 0.9934\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 34s 888ms/step - loss: 0.0142 - accuracy: 0.9975\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 34s 891ms/step - loss: 0.0163 - accuracy: 0.9951\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 0.0125 - accuracy: 0.9959\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 34s 889ms/step - loss: 0.0135 - accuracy: 0.9967\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 0.0128 - accuracy: 0.9951\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 35s 916ms/step - loss: 0.0117 - accuracy: 0.9975\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0167 - accuracy: 0.9959\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 0.0204 - accuracy: 0.9934\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 33s 870ms/step - loss: 0.0130 - accuracy: 0.9975\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 0.0107 - accuracy: 0.9975\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 0.0076 - accuracy: 0.9984\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 31s 811ms/step - loss: 0.0059 - accuracy: 0.9992\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 31s 813ms/step - loss: 0.0075 - accuracy: 0.9984\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.0091 - accuracy: 0.9984\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 0.0096 - accuracy: 0.9975\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.0056 - accuracy: 0.9992\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 32s 849ms/step - loss: 0.0075 - accuracy: 0.9984\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0050 - accuracy: 0.9992\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 0.0124 - accuracy: 0.9951\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0094 - accuracy: 0.9984\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0053 - accuracy: 0.9992\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 0.0054 - accuracy: 0.9992\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 32s 849ms/step - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0026 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0041 - accuracy: 0.9992\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0048 - accuracy: 0.9984\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0181 - accuracy: 0.9951\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 32s 845ms/step - loss: 0.0234 - accuracy: 0.9926\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0081 - accuracy: 0.9984\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 8.7290e-04 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 8.4181e-04 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 32s 852ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 32s 847ms/step - loss: 0.0191 - accuracy: 0.9934\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0141 - accuracy: 0.9942\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 0.0093 - accuracy: 0.9984\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 32s 855ms/step - loss: 6.1155e-04 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 5.3812e-04 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 4.6782e-04 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 4.5817e-04 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 4.3822e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 3.9503e-04 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 3.8277e-04 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 3.2278e-04 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 32s 845ms/step - loss: 6.4587e-04 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 3.4212e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 3.2751e-04 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 2.9040e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 3.2815e-04 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 2.9469e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 2.8224e-04 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 33s 879ms/step - loss: 2.6585e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 3.1430e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 2.6163e-04 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 2.2344e-04 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 3.0334e-04 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 2.4695e-04 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 2.1818e-04 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 32s 829ms/step - loss: 2.4061e-04 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 2.1653e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 2.2962e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 2.0874e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 32s 855ms/step - loss: 1.9475e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 1.7577e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 1.8217e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 1.6961e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 2.0468e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 2.0200e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 36s 958ms/step - loss: 1.6449e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 37s 980ms/step - loss: 1.5489e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 37s 986ms/step - loss: 1.6570e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 36s 940ms/step - loss: 1.4214e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 34s 896ms/step - loss: 1.4774e-04 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 35s 920ms/step - loss: 1.3935e-04 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 35s 921ms/step - loss: 1.5016e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 1.2662e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 34s 886ms/step - loss: 1.3644e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 34s 905ms/step - loss: 1.2568e-04 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 35s 935ms/step - loss: 1.1398e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 36s 944ms/step - loss: 1.1261e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 36s 952ms/step - loss: 1.3559e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 35s 931ms/step - loss: 1.1437e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 35s 913ms/step - loss: 1.0292e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 35s 931ms/step - loss: 1.0064e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 35s 922ms/step - loss: 9.3310e-05 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 1.1447e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 34s 889ms/step - loss: 9.0830e-05 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 34s 899ms/step - loss: 8.9406e-05 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 34s 906ms/step - loss: 9.6308e-05 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 35s 917ms/step - loss: 8.9934e-05 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 35s 918ms/step - loss: 8.3866e-05 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 8.6942e-05 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 7.5890e-05 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 34s 891ms/step - loss: 7.4560e-05 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 7.4474e-05 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 35s 910ms/step - loss: 6.9689e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 35s 908ms/step - loss: 7.7105e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 34s 893ms/step - loss: 6.9185e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 35s 919ms/step - loss: 6.6781e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 34s 896ms/step - loss: 6.2406e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 6.2208e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 34s 901ms/step - loss: 6.4468e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 34s 887ms/step - loss: 5.8690e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 5.6734e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 6.2398e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 34s 881ms/step - loss: 5.8411e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 5.3853e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 1.1235e-04 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 35s 921ms/step - loss: 7.1347e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 35s 929ms/step - loss: 5.2261e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 5.0896e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 5.8133e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 34s 888ms/step - loss: 4.5169e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 34s 904ms/step - loss: 5.6434e-05 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 34s 907ms/step - loss: 9.1208e-05 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 36s 957ms/step - loss: 5.5303e-05 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 4.9316e-05 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 3.9329e-05 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 34s 888ms/step - loss: 4.1614e-05 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 35s 931ms/step - loss: 4.9405e-05 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 37s 981ms/step - loss: 3.6625e-05 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 38s 991ms/step - loss: 3.6575e-05 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 37s 970ms/step - loss: 3.2775e-05 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 37s 984ms/step - loss: 3.2522e-05 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 36s 961ms/step - loss: 3.5217e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 35s 928ms/step - loss: 3.6348e-05 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 35s 928ms/step - loss: 3.3133e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 35s 910ms/step - loss: 3.0990e-05 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 36s 939ms/step - loss: 3.0506e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 35s 918ms/step - loss: 2.7411e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 36s 943ms/step - loss: 3.0085e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 35s 934ms/step - loss: 2.6830e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 36s 936ms/step - loss: 2.8667e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 37s 984ms/step - loss: 2.6423e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 35s 914ms/step - loss: 2.7915e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 34s 889ms/step - loss: 2.5230e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 34s 904ms/step - loss: 2.3619e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 36s 936ms/step - loss: 3.1637e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 35s 922ms/step - loss: 2.5007e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 35s 928ms/step - loss: 2.2656e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 34s 901ms/step - loss: 2.1801e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 2.1127e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 34s 891ms/step - loss: 2.0752e-05 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 2.0267e-05 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 32s 843ms/step - loss: 2.0681e-05 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 2.4735e-05 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 1.8526e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 1.8135e-05 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 1.8224e-05 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "38/38 [==============================] - 34s 896ms/step - loss: 1.8378e-05 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 33s 870ms/step - loss: 1.6411e-05 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 1.7946e-05 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 1.6761e-05 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 1.6300e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 1.5982e-05 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 1.5020e-05 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 1.3091e-05 - accuracy: 1.0000\n",
      "10/10 [==============================] - 2s 200ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "Accuracy:  1.0\n",
      "F1-Score:  1.0\n",
      "\n",
      "True Positives (TP): 118\n",
      "True Negatives (TN): 186\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 0\n",
      "\n",
      "Mean Squared Error: 0.0\n",
      "Training on fold 3/5\n",
      "1215 304\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 33s 846ms/step - loss: 0.8119 - accuracy: 0.6115\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 0.8056 - accuracy: 0.6115\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 0.7967 - accuracy: 0.6115\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.7973 - accuracy: 0.6115\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.7948 - accuracy: 0.6115\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.7958 - accuracy: 0.6115\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 32s 843ms/step - loss: 0.7974 - accuracy: 0.6115\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 32s 841ms/step - loss: 0.7929 - accuracy: 0.6115\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 0.7947 - accuracy: 0.6115\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 0.7932 - accuracy: 0.6115\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 32s 847ms/step - loss: 0.7468 - accuracy: 0.6337\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 0.4880 - accuracy: 0.8140\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 0.3076 - accuracy: 0.9004\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 0.2171 - accuracy: 0.9342\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 0.1793 - accuracy: 0.9457\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 35s 925ms/step - loss: 0.1476 - accuracy: 0.9523\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 35s 931ms/step - loss: 0.1202 - accuracy: 0.9654\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 35s 912ms/step - loss: 0.1045 - accuracy: 0.9728\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 34s 890ms/step - loss: 0.0991 - accuracy: 0.9712\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 34s 897ms/step - loss: 0.0943 - accuracy: 0.9761\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 35s 919ms/step - loss: 0.0801 - accuracy: 0.9770\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 35s 923ms/step - loss: 0.0714 - accuracy: 0.9860\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 35s 920ms/step - loss: 0.0698 - accuracy: 0.9827\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 36s 939ms/step - loss: 0.0594 - accuracy: 0.9868\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 34s 903ms/step - loss: 0.0690 - accuracy: 0.9827\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 34s 901ms/step - loss: 0.0586 - accuracy: 0.9868\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 34s 907ms/step - loss: 0.0530 - accuracy: 0.9877\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 36s 938ms/step - loss: 0.0437 - accuracy: 0.9918\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 35s 908ms/step - loss: 0.0514 - accuracy: 0.9868\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0539 - accuracy: 0.9860\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.0413 - accuracy: 0.9909\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0381 - accuracy: 0.9909\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0555 - accuracy: 0.9860\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 0.0518 - accuracy: 0.9885\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0539 - accuracy: 0.9852\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 32s 841ms/step - loss: 0.0408 - accuracy: 0.9918\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.0365 - accuracy: 0.9909\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 31s 818ms/step - loss: 0.0294 - accuracy: 0.9926\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0339 - accuracy: 0.9918\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0344 - accuracy: 0.9909\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 32s 835ms/step - loss: 0.0241 - accuracy: 0.9926\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 0.0352 - accuracy: 0.9909\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 0.0251 - accuracy: 0.9951\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.0193 - accuracy: 0.9926\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.0238 - accuracy: 0.9934\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.0143 - accuracy: 0.9967\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 0.0157 - accuracy: 0.9967\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 0.0159 - accuracy: 0.9959\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 0.0136 - accuracy: 0.9967\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.0196 - accuracy: 0.9942\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0155 - accuracy: 0.9959\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0092 - accuracy: 0.9984\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 0.0120 - accuracy: 0.9984\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0157 - accuracy: 0.9951\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.0094 - accuracy: 0.9959\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 0.0066 - accuracy: 0.9984\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0075 - accuracy: 0.9984\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.0099 - accuracy: 0.9975\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0053 - accuracy: 0.9992\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 0.0125 - accuracy: 0.9975\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.0051 - accuracy: 0.9992\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 0.0054 - accuracy: 0.9992\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 0.0061 - accuracy: 0.9975\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0051 - accuracy: 0.9984\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 31s 818ms/step - loss: 0.0033 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.0047 - accuracy: 0.9992\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 32s 835ms/step - loss: 0.0092 - accuracy: 0.9984\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 0.0528 - accuracy: 0.9877\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 0.0229 - accuracy: 0.9934\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0064 - accuracy: 0.9984\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.0103 - accuracy: 0.9959\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0047 - accuracy: 0.9984\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 0.0029 - accuracy: 0.9992\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 32s 849ms/step - loss: 0.0022 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 31s 809ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0054 - accuracy: 0.9984\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 0.0057 - accuracy: 0.9975\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 35s 926ms/step - loss: 0.0044 - accuracy: 0.9992\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 36s 948ms/step - loss: 0.0039 - accuracy: 0.9984\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 37s 976ms/step - loss: 0.0044 - accuracy: 0.9984\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 35s 930ms/step - loss: 0.0020 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 40s 1s/step - loss: 9.0779e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 36s 933ms/step - loss: 6.0680e-04 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 36s 953ms/step - loss: 4.8759e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 37s 974ms/step - loss: 5.3969e-04 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 37s 961ms/step - loss: 6.8676e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 37s 974ms/step - loss: 7.1208e-04 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 35s 917ms/step - loss: 7.1327e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 3.9837e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 3.6636e-04 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 3.6007e-04 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 36s 953ms/step - loss: 3.7938e-04 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 37s 963ms/step - loss: 3.6763e-04 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 34s 893ms/step - loss: 3.2892e-04 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 35s 922ms/step - loss: 3.2988e-04 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 39s 1s/step - loss: 3.3232e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 36s 944ms/step - loss: 2.9150e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 35s 927ms/step - loss: 2.7041e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 34s 897ms/step - loss: 2.5317e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 34s 907ms/step - loss: 2.6311e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 38s 998ms/step - loss: 2.4878e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 39s 1s/step - loss: 2.5922e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 39s 1s/step - loss: 2.4555e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 34s 890ms/step - loss: 2.2310e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 33s 865ms/step - loss: 2.1387e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 2.0935e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 34s 883ms/step - loss: 1.9751e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 1.9831e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 2.0413e-04 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 1.9092e-04 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 34s 901ms/step - loss: 2.1300e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 35s 909ms/step - loss: 1.7404e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 34s 893ms/step - loss: 1.6352e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 33s 880ms/step - loss: 1.6811e-04 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 33s 865ms/step - loss: 1.6218e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 1.5636e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 1.8104e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 1.7775e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 1.7318e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 1.4603e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 30s 796ms/step - loss: 1.3463e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 1.3420e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 1.2488e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 1.3747e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 1.2885e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 1.2556e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 30s 795ms/step - loss: 1.2416e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 31s 808ms/step - loss: 1.4872e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 1.7609e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 32s 845ms/step - loss: 1.0737e-04 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 1.1073e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 31s 816ms/step - loss: 1.0967e-04 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 30s 799ms/step - loss: 9.8655e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 31s 809ms/step - loss: 9.4518e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 31s 816ms/step - loss: 9.2530e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 1.0512e-04 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 8.8400e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 30s 801ms/step - loss: 9.0613e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 30s 801ms/step - loss: 8.0509e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 31s 808ms/step - loss: 8.5486e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 7.7022e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 8.0219e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 31s 813ms/step - loss: 7.4191e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 30s 799ms/step - loss: 7.3996e-05 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 31s 809ms/step - loss: 7.2097e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 7.3995e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 6.8019e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 34s 883ms/step - loss: 6.7540e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 34s 906ms/step - loss: 8.5968e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 34s 888ms/step - loss: 7.0150e-05 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 6.2230e-05 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 7.1679e-05 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 34s 888ms/step - loss: 5.9115e-05 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 5.9597e-05 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 5.6221e-05 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 31s 808ms/step - loss: 5.5541e-05 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 5.1862e-05 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 5.1815e-05 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 5.6028e-05 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 6.5703e-05 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 5.9253e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 7.2565e-05 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 5.4935e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 5.4827e-05 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 6.5307e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 4.7288e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 4.0205e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 3.9695e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 32s 855ms/step - loss: 4.2598e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 3.7731e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 3.6953e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 35s 912ms/step - loss: 3.9290e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 3.5438e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 3.6037e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 3.3006e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 3.7458e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 3.0566e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 3.5537e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 3.3993e-05 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 35s 920ms/step - loss: 2.7913e-05 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 3.0785e-05 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 34s 897ms/step - loss: 2.9189e-05 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 37s 962ms/step - loss: 2.7911e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 37s 958ms/step - loss: 2.6708e-05 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 34s 898ms/step - loss: 2.5318e-05 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "38/38 [==============================] - 34s 905ms/step - loss: 2.8171e-05 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 36s 937ms/step - loss: 3.3648e-05 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 34s 903ms/step - loss: 3.3645e-05 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 34s 905ms/step - loss: 3.3733e-05 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 35s 914ms/step - loss: 2.4936e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 2.2777e-05 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 2.1696e-05 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 2.3983e-05 - accuracy: 1.0000\n",
      "10/10 [==============================] - 2s 201ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision:  0.9919354838709677\n",
      "Recall:  0.9919354838709677\n",
      "Accuracy:  0.993421052631579\n",
      "F1-Score:  0.9919354838709677\n",
      "\n",
      "True Positives (TP): 123\n",
      "True Negatives (TN): 179\n",
      "False Positives (FP): 1\n",
      "False Negatives (FN): 1\n",
      "\n",
      "Mean Squared Error: 0.006578947368421052\n",
      "Training on fold 4/5\n",
      "1215 304\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.8201 - accuracy: 0.6140\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 0.7961 - accuracy: 0.6140\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 0.7947 - accuracy: 0.6140\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.7961 - accuracy: 0.6140\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 0.7955 - accuracy: 0.6140\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.7934 - accuracy: 0.6140\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 0.7941 - accuracy: 0.6140\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 34s 889ms/step - loss: 0.7970 - accuracy: 0.6140\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 34s 903ms/step - loss: 0.7970 - accuracy: 0.6140\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.7957 - accuracy: 0.6140\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 33s 871ms/step - loss: 0.7908 - accuracy: 0.6140\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.7794 - accuracy: 0.6140\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 34s 895ms/step - loss: 0.6853 - accuracy: 0.6560\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 34s 902ms/step - loss: 0.3998 - accuracy: 0.8716\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 34s 890ms/step - loss: 0.2521 - accuracy: 0.9292\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 33s 870ms/step - loss: 0.1829 - accuracy: 0.9473\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 34s 898ms/step - loss: 0.1674 - accuracy: 0.9481\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 0.1448 - accuracy: 0.9588\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 32s 839ms/step - loss: 0.1244 - accuracy: 0.9654\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 32s 831ms/step - loss: 0.1077 - accuracy: 0.9704\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.1104 - accuracy: 0.9695\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.1034 - accuracy: 0.9695\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 0.0920 - accuracy: 0.9786\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.0985 - accuracy: 0.9737\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 0.0752 - accuracy: 0.9827\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0737 - accuracy: 0.9819\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 31s 806ms/step - loss: 0.0642 - accuracy: 0.9844\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 0.0581 - accuracy: 0.9860\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.0525 - accuracy: 0.9877\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 33s 858ms/step - loss: 0.0543 - accuracy: 0.9877\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0526 - accuracy: 0.9868\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0443 - accuracy: 0.9877\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 31s 805ms/step - loss: 0.0416 - accuracy: 0.9901\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 31s 811ms/step - loss: 0.0383 - accuracy: 0.9885\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0350 - accuracy: 0.9918\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0395 - accuracy: 0.9877\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 32s 834ms/step - loss: 0.0546 - accuracy: 0.9835\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.0336 - accuracy: 0.9926\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 31s 806ms/step - loss: 0.0307 - accuracy: 0.9934\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.0349 - accuracy: 0.9901\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.0249 - accuracy: 0.9959\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0343 - accuracy: 0.9909\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 32s 841ms/step - loss: 0.0244 - accuracy: 0.9934\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 32s 835ms/step - loss: 0.0472 - accuracy: 0.9877\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.0260 - accuracy: 0.9934\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 31s 806ms/step - loss: 0.0207 - accuracy: 0.9942\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 30s 800ms/step - loss: 0.0288 - accuracy: 0.9934\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0168 - accuracy: 0.9951\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 0.0149 - accuracy: 0.9959\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 33s 874ms/step - loss: 0.0116 - accuracy: 0.9967\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 35s 915ms/step - loss: 0.0147 - accuracy: 0.9951\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 35s 931ms/step - loss: 0.0118 - accuracy: 0.9967\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 0.0162 - accuracy: 0.9951\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 0.0163 - accuracy: 0.9959\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 35s 918ms/step - loss: 0.0152 - accuracy: 0.9959\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 37s 976ms/step - loss: 0.0110 - accuracy: 0.9975\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 36s 946ms/step - loss: 0.0121 - accuracy: 0.9975\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 39s 1s/step - loss: 0.0115 - accuracy: 0.9967\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 38s 993ms/step - loss: 0.0123 - accuracy: 0.9951\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 37s 975ms/step - loss: 0.0056 - accuracy: 0.9992\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 35s 910ms/step - loss: 0.0088 - accuracy: 0.9975\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 34s 904ms/step - loss: 0.0055 - accuracy: 0.9984\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 35s 916ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 38s 999ms/step - loss: 0.0039 - accuracy: 0.9992\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 36s 937ms/step - loss: 0.0039 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0062 - accuracy: 0.9984\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 0.0109 - accuracy: 0.9959\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 36s 938ms/step - loss: 0.0045 - accuracy: 0.9992\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 34s 882ms/step - loss: 0.0076 - accuracy: 0.9967\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 0.0061 - accuracy: 0.9975\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 0.0054 - accuracy: 0.9992\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 0.0081 - accuracy: 0.9984\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 35s 911ms/step - loss: 0.0032 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 36s 938ms/step - loss: 0.0024 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 36s 935ms/step - loss: 0.0021 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 35s 910ms/step - loss: 0.0019 - accuracy: 0.9992\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 34s 908ms/step - loss: 0.0083 - accuracy: 0.9975\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 0.0030 - accuracy: 0.9992\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 35s 913ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 34s 882ms/step - loss: 0.0011 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 8.8350e-04 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 8.8132e-04 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 8.7721e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 32s 845ms/step - loss: 8.0568e-04 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 32s 848ms/step - loss: 8.3275e-04 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 36s 941ms/step - loss: 0.0013 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 33s 879ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 34s 902ms/step - loss: 7.2701e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 38s 995ms/step - loss: 6.1850e-04 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 35s 929ms/step - loss: 5.5739e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 34s 897ms/step - loss: 5.2328e-04 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 5.4650e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 35s 918ms/step - loss: 4.9784e-04 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 5.3350e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 37s 965ms/step - loss: 5.3797e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 37s 963ms/step - loss: 4.3107e-04 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 36s 950ms/step - loss: 4.3609e-04 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 37s 975ms/step - loss: 3.7432e-04 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 36s 941ms/step - loss: 4.6525e-04 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 38s 990ms/step - loss: 4.5422e-04 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 35s 912ms/step - loss: 4.0963e-04 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 36s 963ms/step - loss: 3.2480e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 36s 946ms/step - loss: 3.5713e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 33s 855ms/step - loss: 5.2682e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 34s 892ms/step - loss: 3.4879e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 35s 915ms/step - loss: 3.5610e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 3.7943e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 32s 847ms/step - loss: 2.7726e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 3.0844e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 31s 813ms/step - loss: 2.7398e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 2.8180e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 32s 835ms/step - loss: 2.0839e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 2.8091e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 2.9473e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 31s 803ms/step - loss: 3.1917e-04 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 2.7995e-04 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 31s 818ms/step - loss: 1.9172e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 32s 829ms/step - loss: 1.8018e-04 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 32s 832ms/step - loss: 1.9962e-04 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 1.7626e-04 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 1.5538e-04 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 32s 846ms/step - loss: 1.7253e-04 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 2.2253e-04 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 1.5315e-04 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 33s 883ms/step - loss: 1.5738e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 34s 894ms/step - loss: 1.5106e-04 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 36s 951ms/step - loss: 1.3308e-04 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 32s 836ms/step - loss: 1.3994e-04 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 1.5119e-04 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 36s 961ms/step - loss: 1.2271e-04 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 35s 929ms/step - loss: 1.1960e-04 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 35s 907ms/step - loss: 1.1148e-04 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 31s 830ms/step - loss: 1.1181e-04 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 34s 900ms/step - loss: 1.0964e-04 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 35s 910ms/step - loss: 1.1059e-04 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 33s 857ms/step - loss: 9.4915e-05 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 34s 908ms/step - loss: 1.0800e-04 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 36s 935ms/step - loss: 1.2806e-04 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 36s 947ms/step - loss: 8.8293e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 36s 941ms/step - loss: 8.4630e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 35s 920ms/step - loss: 2.1547e-04 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 0.0257 - accuracy: 0.9959\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 34s 889ms/step - loss: 0.0643 - accuracy: 0.9868\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 36s 939ms/step - loss: 0.0518 - accuracy: 0.9860\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 34s 907ms/step - loss: 0.0095 - accuracy: 0.9984\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 36s 937ms/step - loss: 0.0050 - accuracy: 0.9984\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.0016 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 32s 844ms/step - loss: 8.9146e-04 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 32s 842ms/step - loss: 3.9667e-04 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 32s 838ms/step - loss: 4.3000e-04 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 38s 988ms/step - loss: 4.1655e-04 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 39s 1s/step - loss: 3.2669e-04 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 36s 957ms/step - loss: 2.8603e-04 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 38s 1s/step - loss: 2.3589e-04 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 37s 965ms/step - loss: 2.4823e-04 - accuracy: 1.0000\n",
      "Epoch 156: early stopping\n",
      "10/10 [==============================] - 2s 194ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision:  1.0\n",
      "Recall:  0.9606299212598425\n",
      "Accuracy:  0.9835526315789473\n",
      "F1-Score:  0.9799196787148594\n",
      "\n",
      "True Positives (TP): 122\n",
      "True Negatives (TN): 177\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 5\n",
      "\n",
      "Mean Squared Error: 0.01644736842105263\n",
      "Training on fold 5/5\n",
      "1216 303\n",
      "Epoch 1/200\n",
      "38/38 [==============================] - 34s 872ms/step - loss: 0.8175 - accuracy: 0.5979\n",
      "Epoch 2/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 0.8049 - accuracy: 0.5979\n",
      "Epoch 3/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 0.8023 - accuracy: 0.5979\n",
      "Epoch 4/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.8054 - accuracy: 0.5979\n",
      "Epoch 5/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 0.8034 - accuracy: 0.5979\n",
      "Epoch 6/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 0.8014 - accuracy: 0.5979\n",
      "Epoch 7/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.7999 - accuracy: 0.5979\n",
      "Epoch 8/200\n",
      "38/38 [==============================] - 32s 828ms/step - loss: 0.7951 - accuracy: 0.5979\n",
      "Epoch 9/200\n",
      "38/38 [==============================] - 31s 826ms/step - loss: 0.7671 - accuracy: 0.6012\n",
      "Epoch 10/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.6297 - accuracy: 0.7105\n",
      "Epoch 11/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.3112 - accuracy: 0.9120\n",
      "Epoch 12/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 0.1863 - accuracy: 0.9465\n",
      "Epoch 13/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.1291 - accuracy: 0.9679\n",
      "Epoch 14/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 0.1051 - accuracy: 0.9679\n",
      "Epoch 15/200\n",
      "38/38 [==============================] - 31s 818ms/step - loss: 0.0926 - accuracy: 0.9729\n",
      "Epoch 16/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.0797 - accuracy: 0.9778\n",
      "Epoch 17/200\n",
      "38/38 [==============================] - 32s 833ms/step - loss: 0.0596 - accuracy: 0.9836\n",
      "Epoch 18/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 0.0663 - accuracy: 0.9819\n",
      "Epoch 19/200\n",
      "38/38 [==============================] - 30s 800ms/step - loss: 0.0572 - accuracy: 0.9877\n",
      "Epoch 20/200\n",
      "38/38 [==============================] - 31s 827ms/step - loss: 0.0565 - accuracy: 0.9827\n",
      "Epoch 21/200\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0548 - accuracy: 0.9844\n",
      "Epoch 22/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0475 - accuracy: 0.9885\n",
      "Epoch 23/200\n",
      "38/38 [==============================] - 32s 835ms/step - loss: 0.0449 - accuracy: 0.9868\n",
      "Epoch 24/200\n",
      "38/38 [==============================] - 32s 837ms/step - loss: 0.0528 - accuracy: 0.9844\n",
      "Epoch 25/200\n",
      "38/38 [==============================] - 31s 819ms/step - loss: 0.0317 - accuracy: 0.9918\n",
      "Epoch 26/200\n",
      "38/38 [==============================] - 31s 803ms/step - loss: 0.0253 - accuracy: 0.9942\n",
      "Epoch 27/200\n",
      "38/38 [==============================] - 31s 804ms/step - loss: 0.0295 - accuracy: 0.9951\n",
      "Epoch 28/200\n",
      "38/38 [==============================] - 31s 810ms/step - loss: 0.0314 - accuracy: 0.9934\n",
      "Epoch 29/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.0263 - accuracy: 0.9942\n",
      "Epoch 30/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0292 - accuracy: 0.9893\n",
      "Epoch 31/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 0.0349 - accuracy: 0.9918\n",
      "Epoch 32/200\n",
      "38/38 [==============================] - 31s 807ms/step - loss: 0.0218 - accuracy: 0.9942\n",
      "Epoch 33/200\n",
      "38/38 [==============================] - 31s 803ms/step - loss: 0.0165 - accuracy: 0.9967\n",
      "Epoch 34/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 0.0164 - accuracy: 0.9959\n",
      "Epoch 35/200\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0162 - accuracy: 0.9959\n",
      "Epoch 36/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.0113 - accuracy: 0.9984\n",
      "Epoch 37/200\n",
      "38/38 [==============================] - 32s 840ms/step - loss: 0.0106 - accuracy: 0.9975\n",
      "Epoch 38/200\n",
      "38/38 [==============================] - 31s 816ms/step - loss: 0.0153 - accuracy: 0.9951\n",
      "Epoch 39/200\n",
      "38/38 [==============================] - 30s 802ms/step - loss: 0.0195 - accuracy: 0.9934\n",
      "Epoch 40/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0347 - accuracy: 0.9893\n",
      "Epoch 41/200\n",
      "38/38 [==============================] - 31s 821ms/step - loss: 0.0115 - accuracy: 0.9959\n",
      "Epoch 42/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 0.0093 - accuracy: 0.9975\n",
      "Epoch 43/200\n",
      "38/38 [==============================] - 31s 812ms/step - loss: 0.0076 - accuracy: 0.9984\n",
      "Epoch 44/200\n",
      "38/38 [==============================] - 31s 825ms/step - loss: 0.0093 - accuracy: 0.9984\n",
      "Epoch 45/200\n",
      "38/38 [==============================] - 31s 828ms/step - loss: 0.0108 - accuracy: 0.9951\n",
      "Epoch 46/200\n",
      "38/38 [==============================] - 31s 813ms/step - loss: 0.0078 - accuracy: 0.9984\n",
      "Epoch 47/200\n",
      "38/38 [==============================] - 30s 799ms/step - loss: 0.0078 - accuracy: 0.9975\n",
      "Epoch 48/200\n",
      "38/38 [==============================] - 30s 801ms/step - loss: 0.0091 - accuracy: 0.9975\n",
      "Epoch 49/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 0.0034 - accuracy: 0.9992\n",
      "Epoch 50/200\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 51/200\n",
      "38/38 [==============================] - 31s 824ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 52/200\n",
      "38/38 [==============================] - 32s 830ms/step - loss: 0.0028 - accuracy: 1.0000\n",
      "Epoch 53/200\n",
      "38/38 [==============================] - 31s 822ms/step - loss: 0.0085 - accuracy: 0.9975\n",
      "Epoch 54/200\n",
      "38/38 [==============================] - 30s 802ms/step - loss: 0.0029 - accuracy: 1.0000\n",
      "Epoch 55/200\n",
      "38/38 [==============================] - 31s 809ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 56/200\n",
      "38/38 [==============================] - 31s 823ms/step - loss: 0.0022 - accuracy: 0.9992\n",
      "Epoch 57/200\n",
      "38/38 [==============================] - 31s 817ms/step - loss: 0.0017 - accuracy: 1.0000\n",
      "Epoch 58/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 0.0014 - accuracy: 1.0000\n",
      "Epoch 59/200\n",
      "38/38 [==============================] - 31s 815ms/step - loss: 0.0046 - accuracy: 0.9984\n",
      "Epoch 60/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 0.0072 - accuracy: 0.9992\n",
      "Epoch 61/200\n",
      "38/38 [==============================] - 31s 803ms/step - loss: 0.0036 - accuracy: 1.0000\n",
      "Epoch 62/200\n",
      "38/38 [==============================] - 31s 806ms/step - loss: 0.0027 - accuracy: 0.9992\n",
      "Epoch 63/200\n",
      "38/38 [==============================] - 31s 811ms/step - loss: 0.0019 - accuracy: 1.0000\n",
      "Epoch 64/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.0012 - accuracy: 1.0000\n",
      "Epoch 65/200\n",
      "38/38 [==============================] - 31s 829ms/step - loss: 9.5057e-04 - accuracy: 1.0000\n",
      "Epoch 66/200\n",
      "38/38 [==============================] - 31s 820ms/step - loss: 0.0010 - accuracy: 1.0000\n",
      "Epoch 67/200\n",
      "38/38 [==============================] - 31s 814ms/step - loss: 9.9124e-04 - accuracy: 1.0000\n",
      "Epoch 68/200\n",
      "38/38 [==============================] - 31s 811ms/step - loss: 6.4439e-04 - accuracy: 1.0000\n",
      "Epoch 69/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 5.8777e-04 - accuracy: 1.0000\n",
      "Epoch 70/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 5.6779e-04 - accuracy: 1.0000\n",
      "Epoch 71/200\n",
      "38/38 [==============================] - 33s 864ms/step - loss: 5.7804e-04 - accuracy: 1.0000\n",
      "Epoch 72/200\n",
      "38/38 [==============================] - 32s 852ms/step - loss: 8.8555e-04 - accuracy: 1.0000\n",
      "Epoch 73/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 7.1288e-04 - accuracy: 1.0000\n",
      "Epoch 74/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 4.7571e-04 - accuracy: 1.0000\n",
      "Epoch 75/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 4.7123e-04 - accuracy: 1.0000\n",
      "Epoch 76/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 6.0557e-04 - accuracy: 1.0000\n",
      "Epoch 77/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 4.4053e-04 - accuracy: 1.0000\n",
      "Epoch 78/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 3.7631e-04 - accuracy: 1.0000\n",
      "Epoch 79/200\n",
      "38/38 [==============================] - 33s 878ms/step - loss: 4.5547e-04 - accuracy: 1.0000\n",
      "Epoch 80/200\n",
      "38/38 [==============================] - 34s 884ms/step - loss: 5.3894e-04 - accuracy: 1.0000\n",
      "Epoch 81/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 6.6750e-04 - accuracy: 1.0000\n",
      "Epoch 82/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 3.5541e-04 - accuracy: 1.0000\n",
      "Epoch 83/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 3.4248e-04 - accuracy: 1.0000\n",
      "Epoch 84/200\n",
      "38/38 [==============================] - 34s 882ms/step - loss: 3.2425e-04 - accuracy: 1.0000\n",
      "Epoch 85/200\n",
      "38/38 [==============================] - 33s 870ms/step - loss: 2.8984e-04 - accuracy: 1.0000\n",
      "Epoch 86/200\n",
      "38/38 [==============================] - 33s 864ms/step - loss: 3.1742e-04 - accuracy: 1.0000\n",
      "Epoch 87/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 3.0560e-04 - accuracy: 1.0000\n",
      "Epoch 88/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 3.7159e-04 - accuracy: 1.0000\n",
      "Epoch 89/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 2.6190e-04 - accuracy: 1.0000\n",
      "Epoch 90/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 2.4483e-04 - accuracy: 1.0000\n",
      "Epoch 91/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 2.5479e-04 - accuracy: 1.0000\n",
      "Epoch 92/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 2.3263e-04 - accuracy: 1.0000\n",
      "Epoch 93/200\n",
      "38/38 [==============================] - 34s 885ms/step - loss: 2.1921e-04 - accuracy: 1.0000\n",
      "Epoch 94/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 2.0180e-04 - accuracy: 1.0000\n",
      "Epoch 95/200\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 1.9904e-04 - accuracy: 1.0000\n",
      "Epoch 96/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 2.0539e-04 - accuracy: 1.0000\n",
      "Epoch 97/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 2.0926e-04 - accuracy: 1.0000\n",
      "Epoch 98/200\n",
      "38/38 [==============================] - 33s 874ms/step - loss: 1.8607e-04 - accuracy: 1.0000\n",
      "Epoch 99/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 1.7602e-04 - accuracy: 1.0000\n",
      "Epoch 100/200\n",
      "38/38 [==============================] - 33s 855ms/step - loss: 1.6885e-04 - accuracy: 1.0000\n",
      "Epoch 101/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 1.7221e-04 - accuracy: 1.0000\n",
      "Epoch 102/200\n",
      "38/38 [==============================] - 33s 879ms/step - loss: 1.5480e-04 - accuracy: 1.0000\n",
      "Epoch 103/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 1.7638e-04 - accuracy: 1.0000\n",
      "Epoch 104/200\n",
      "38/38 [==============================] - 33s 880ms/step - loss: 1.5143e-04 - accuracy: 1.0000\n",
      "Epoch 105/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 1.5633e-04 - accuracy: 1.0000\n",
      "Epoch 106/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 2.0807e-04 - accuracy: 1.0000\n",
      "Epoch 107/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 1.4085e-04 - accuracy: 1.0000\n",
      "Epoch 108/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 1.3972e-04 - accuracy: 1.0000\n",
      "Epoch 109/200\n",
      "38/38 [==============================] - 34s 886ms/step - loss: 1.2852e-04 - accuracy: 1.0000\n",
      "Epoch 110/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 1.2028e-04 - accuracy: 1.0000\n",
      "Epoch 111/200\n",
      "38/38 [==============================] - 33s 864ms/step - loss: 1.1689e-04 - accuracy: 1.0000\n",
      "Epoch 112/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 1.3310e-04 - accuracy: 1.0000\n",
      "Epoch 113/200\n",
      "38/38 [==============================] - 33s 871ms/step - loss: 1.1297e-04 - accuracy: 1.0000\n",
      "Epoch 114/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 1.2172e-04 - accuracy: 1.0000\n",
      "Epoch 115/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 1.0393e-04 - accuracy: 1.0000\n",
      "Epoch 116/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 9.8992e-05 - accuracy: 1.0000\n",
      "Epoch 117/200\n",
      "38/38 [==============================] - 33s 858ms/step - loss: 8.5953e-05 - accuracy: 1.0000\n",
      "Epoch 118/200\n",
      "38/38 [==============================] - 33s 870ms/step - loss: 1.0410e-04 - accuracy: 1.0000\n",
      "Epoch 119/200\n",
      "38/38 [==============================] - 33s 870ms/step - loss: 9.3935e-05 - accuracy: 1.0000\n",
      "Epoch 120/200\n",
      "38/38 [==============================] - 34s 883ms/step - loss: 8.3189e-05 - accuracy: 1.0000\n",
      "Epoch 121/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 8.7723e-05 - accuracy: 1.0000\n",
      "Epoch 122/200\n",
      "38/38 [==============================] - 32s 855ms/step - loss: 8.0146e-05 - accuracy: 1.0000\n",
      "Epoch 123/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 7.6357e-05 - accuracy: 1.0000\n",
      "Epoch 124/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 7.5817e-05 - accuracy: 1.0000\n",
      "Epoch 125/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 7.7572e-05 - accuracy: 1.0000\n",
      "Epoch 126/200\n",
      "38/38 [==============================] - 33s 879ms/step - loss: 1.0810e-04 - accuracy: 1.0000\n",
      "Epoch 127/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 7.3732e-05 - accuracy: 1.0000\n",
      "Epoch 128/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 6.6213e-05 - accuracy: 1.0000\n",
      "Epoch 129/200\n",
      "38/38 [==============================] - 33s 858ms/step - loss: 6.7465e-05 - accuracy: 1.0000\n",
      "Epoch 130/200\n",
      "38/38 [==============================] - 33s 865ms/step - loss: 6.1122e-05 - accuracy: 1.0000\n",
      "Epoch 131/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 6.2449e-05 - accuracy: 1.0000\n",
      "Epoch 132/200\n",
      "38/38 [==============================] - 34s 882ms/step - loss: 6.0970e-05 - accuracy: 1.0000\n",
      "Epoch 133/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 5.9212e-05 - accuracy: 1.0000\n",
      "Epoch 134/200\n",
      "38/38 [==============================] - 32s 849ms/step - loss: 5.8943e-05 - accuracy: 1.0000\n",
      "Epoch 135/200\n",
      "38/38 [==============================] - 33s 857ms/step - loss: 5.7382e-05 - accuracy: 1.0000\n",
      "Epoch 136/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 6.1624e-05 - accuracy: 1.0000\n",
      "Epoch 137/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 5.0217e-05 - accuracy: 1.0000\n",
      "Epoch 138/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 5.1114e-05 - accuracy: 1.0000\n",
      "Epoch 139/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 4.9009e-05 - accuracy: 1.0000\n",
      "Epoch 140/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 4.7490e-05 - accuracy: 1.0000\n",
      "Epoch 141/200\n",
      "38/38 [==============================] - 33s 857ms/step - loss: 4.4031e-05 - accuracy: 1.0000\n",
      "Epoch 142/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 4.9750e-05 - accuracy: 1.0000\n",
      "Epoch 143/200\n",
      "38/38 [==============================] - 33s 873ms/step - loss: 6.0475e-05 - accuracy: 1.0000\n",
      "Epoch 144/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 4.7978e-05 - accuracy: 1.0000\n",
      "Epoch 145/200\n",
      "38/38 [==============================] - 33s 869ms/step - loss: 4.1017e-05 - accuracy: 1.0000\n",
      "Epoch 146/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 4.0421e-05 - accuracy: 1.0000\n",
      "Epoch 147/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 3.8182e-05 - accuracy: 1.0000\n",
      "Epoch 148/200\n",
      "38/38 [==============================] - 33s 881ms/step - loss: 3.9519e-05 - accuracy: 1.0000\n",
      "Epoch 149/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 3.6354e-05 - accuracy: 1.0000\n",
      "Epoch 150/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 3.6736e-05 - accuracy: 1.0000\n",
      "Epoch 151/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 4.2558e-05 - accuracy: 1.0000\n",
      "Epoch 152/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 3.4679e-05 - accuracy: 1.0000\n",
      "Epoch 153/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 3.3734e-05 - accuracy: 1.0000\n",
      "Epoch 154/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 3.1091e-05 - accuracy: 1.0000\n",
      "Epoch 155/200\n",
      "38/38 [==============================] - 33s 877ms/step - loss: 2.9964e-05 - accuracy: 1.0000\n",
      "Epoch 156/200\n",
      "38/38 [==============================] - 33s 861ms/step - loss: 3.3867e-05 - accuracy: 1.0000\n",
      "Epoch 157/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 3.0288e-05 - accuracy: 1.0000\n",
      "Epoch 158/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 2.7775e-05 - accuracy: 1.0000\n",
      "Epoch 159/200\n",
      "38/38 [==============================] - 33s 864ms/step - loss: 3.3672e-05 - accuracy: 1.0000\n",
      "Epoch 160/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 2.6138e-05 - accuracy: 1.0000\n",
      "Epoch 161/200\n",
      "38/38 [==============================] - 33s 868ms/step - loss: 3.1980e-05 - accuracy: 1.0000\n",
      "Epoch 162/200\n",
      "38/38 [==============================] - 33s 857ms/step - loss: 2.4062e-05 - accuracy: 1.0000\n",
      "Epoch 163/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 2.7394e-05 - accuracy: 1.0000\n",
      "Epoch 164/200\n",
      "38/38 [==============================] - 33s 876ms/step - loss: 2.7040e-05 - accuracy: 1.0000\n",
      "Epoch 165/200\n",
      "38/38 [==============================] - 34s 901ms/step - loss: 2.3869e-05 - accuracy: 1.0000\n",
      "Epoch 166/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 2.3556e-05 - accuracy: 1.0000\n",
      "Epoch 167/200\n",
      "38/38 [==============================] - 33s 866ms/step - loss: 2.3260e-05 - accuracy: 1.0000\n",
      "Epoch 168/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 2.2716e-05 - accuracy: 1.0000\n",
      "Epoch 169/200\n",
      "38/38 [==============================] - 33s 865ms/step - loss: 2.6217e-05 - accuracy: 1.0000\n",
      "Epoch 170/200\n",
      "38/38 [==============================] - 33s 872ms/step - loss: 2.2678e-05 - accuracy: 1.0000\n",
      "Epoch 171/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 2.1774e-05 - accuracy: 1.0000\n",
      "Epoch 172/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 2.0513e-05 - accuracy: 1.0000\n",
      "Epoch 173/200\n",
      "38/38 [==============================] - 33s 863ms/step - loss: 1.9915e-05 - accuracy: 1.0000\n",
      "Epoch 174/200\n",
      "38/38 [==============================] - 33s 867ms/step - loss: 1.9376e-05 - accuracy: 1.0000\n",
      "Epoch 175/200\n",
      "38/38 [==============================] - 33s 875ms/step - loss: 1.7955e-05 - accuracy: 1.0000\n",
      "Epoch 176/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 1.7348e-05 - accuracy: 1.0000\n",
      "Epoch 177/200\n",
      "38/38 [==============================] - 33s 855ms/step - loss: 1.8423e-05 - accuracy: 1.0000\n",
      "Epoch 178/200\n",
      "38/38 [==============================] - 32s 852ms/step - loss: 1.6954e-05 - accuracy: 1.0000\n",
      "Epoch 179/200\n",
      "38/38 [==============================] - 33s 864ms/step - loss: 1.8902e-05 - accuracy: 1.0000\n",
      "Epoch 180/200\n",
      "38/38 [==============================] - 33s 871ms/step - loss: 1.8106e-05 - accuracy: 1.0000\n",
      "Epoch 181/200\n",
      "38/38 [==============================] - 33s 864ms/step - loss: 1.6017e-05 - accuracy: 1.0000\n",
      "Epoch 182/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 1.5408e-05 - accuracy: 1.0000\n",
      "Epoch 183/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 1.4571e-05 - accuracy: 1.0000\n",
      "Epoch 184/200\n",
      "38/38 [==============================] - 33s 860ms/step - loss: 1.4475e-05 - accuracy: 1.0000\n",
      "Epoch 185/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 1.3614e-05 - accuracy: 1.0000\n",
      "Epoch 186/200\n",
      "38/38 [==============================] - 33s 858ms/step - loss: 1.3466e-05 - accuracy: 1.0000\n",
      "Epoch 187/200\n",
      "38/38 [==============================] - 32s 851ms/step - loss: 1.3937e-05 - accuracy: 1.0000\n",
      "Epoch 188/200\n",
      "38/38 [==============================] - 33s 858ms/step - loss: 1.3281e-05 - accuracy: 1.0000\n",
      "Epoch 189/200\n",
      "38/38 [==============================] - 33s 859ms/step - loss: 1.3316e-05 - accuracy: 1.0000\n",
      "Epoch 190/200\n",
      "38/38 [==============================] - 33s 856ms/step - loss: 1.1541e-05 - accuracy: 1.0000\n",
      "Epoch 191/200\n",
      "38/38 [==============================] - 33s 857ms/step - loss: 1.2347e-05 - accuracy: 1.0000\n",
      "Epoch 192/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 1.3039e-05 - accuracy: 1.0000\n",
      "Epoch 193/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 1.3391e-05 - accuracy: 1.0000\n",
      "Epoch 194/200\n",
      "38/38 [==============================] - 32s 850ms/step - loss: 1.2370e-05 - accuracy: 1.0000\n",
      "Epoch 195/200\n",
      "38/38 [==============================] - 32s 855ms/step - loss: 1.0659e-05 - accuracy: 1.0000\n",
      "Epoch 196/200\n",
      "38/38 [==============================] - 32s 855ms/step - loss: 1.0456e-05 - accuracy: 1.0000\n",
      "Epoch 197/200\n",
      "38/38 [==============================] - 32s 854ms/step - loss: 1.0894e-05 - accuracy: 1.0000\n",
      "Epoch 198/200\n",
      "38/38 [==============================] - 32s 853ms/step - loss: 9.6423e-06 - accuracy: 1.0000\n",
      "Epoch 199/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 1.0400e-05 - accuracy: 1.0000\n",
      "Epoch 200/200\n",
      "38/38 [==============================] - 33s 862ms/step - loss: 1.0138e-05 - accuracy: 1.0000\n",
      "10/10 [==============================] - 2s 198ms/step\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Precision:  0.9809523809523809\n",
      "Recall:  0.9626168224299065\n",
      "Accuracy:  0.9801980198019802\n",
      "F1-Score:  0.9716981132075471\n",
      "\n",
      "True Positives (TP): 103\n",
      "True Negatives (TN): 194\n",
      "False Positives (FP): 2\n",
      "False Negatives (FN): 4\n",
      "\n",
      "Mean Squared Error: 0.019801980198019802\n"
     ]
    }
   ],
   "source": [
    "totFold = 5\n",
    "\n",
    "kf = KFold(n_splits=totFold, shuffle=True, random_state=42)\n",
    "\n",
    "arrHist = []\n",
    "models = []\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "cms = []\n",
    "mse_scores = []\n",
    "\n",
    "train_indexes = []\n",
    "test_indexes = []\n",
    "\n",
    "fold_num = 1\n",
    "KFoldSplitted = kf.split(X)\n",
    "\n",
    "for train_index, test_index in KFoldSplitted:\n",
    "    print(f\"Training on fold {fold_num}/{totFold}\")\n",
    "\n",
    "    print(len(train_index), len(test_index))\n",
    "    train_indexes.append(train_index)\n",
    "    test_indexes.append(test_index)\n",
    "\n",
    "    # Splitting the data for this fold\n",
    "    X_train_fold, X_test_fold = X[train_index], X[test_index]\n",
    "    y_train_fold, y_test_fold = y[train_index], y[test_index]\n",
    "\n",
    "\n",
    "    model = getModel()\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.0001)\n",
    "    model.compile(optimizer=optimizer, loss=tf.losses.BinaryCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "    early_stop = EarlyStopping(monitor='loss', patience=15, verbose=1)\n",
    "\n",
    "    class_weights = {0: 1.5,  # noise\n",
    "                     1: 1.0}  # dolphin's whistle\n",
    "\n",
    "    # Continue training the model\n",
    "    hist = model.fit(X_train_fold, y_train_fold, batch_size=32, epochs=200, class_weight=class_weights, callbacks=[early_stop])\n",
    "\n",
    "    arrHist.append(hist)\n",
    "    models.append(model)\n",
    "\n",
    "\n",
    "    y_pred = model.predict(X_test_fold)\n",
    "    y_pred_binary = (y_pred > 0.5).astype(np.int32)  # Convert probabilities to binary predictions\n",
    "\n",
    "    # Calculate precision and recall\n",
    "    precisions.append(precision_score(y_test_fold, y_pred_binary))\n",
    "    recalls.append(recall_score(y_test_fold, y_pred_binary))\n",
    "    accuracies.append(accuracy_score(y_test_fold, y_pred_binary))\n",
    "    f1_scores.append(f1_score(y_test_fold, y_pred_binary))\n",
    "\n",
    "    cm = confusion_matrix(y_test_fold, y_pred_binary)\n",
    "    cms.append(cm)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "    # Calculate the mean squared error\n",
    "    mse = mean_squared_error(y_test_fold, y_pred_binary)\n",
    "    mse_scores.append(mse)\n",
    "\n",
    "    print('\\n\\n\\n')\n",
    "\n",
    "    print('Precision: ', precisions[fold_num-1])\n",
    "    print('Recall: ', recalls[fold_num-1])\n",
    "    print('Accuracy: ', accuracies[fold_num-1])\n",
    "    print('F1-Score: ', f1_scores[fold_num-1])\n",
    "    print()\n",
    "    print(f\"True Positives (TP): {TP}\")\n",
    "    print(f\"True Negatives (TN): {TN}\")\n",
    "    print(f\"False Positives (FP): {FP}\")\n",
    "    print(f\"False Negatives (FN): {FN}\")\n",
    "    print()\n",
    "    print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "\n",
    "    # Increment the fold number for the next iteration\n",
    "    fold_num += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T18:55:55.498421800Z",
     "start_time": "2023-09-07T10:10:26.878390400Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 5) Valutazione modello"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.1) Valutazione rete senza fold cross validation\n",
    "Inserendo una soglia a 0.5, vengono processate dalla rete neurale tutte le immagini presenti nel dataset di test. Dopodichè vengono calcolate le metriche di valutazione Precision, Recall, Accuracy, F1-Score e MSE. Inoltre, viene calcolata anche la Confusion Matrix che riporta il numero di TP,TN,FP,FN.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Make predictions on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_binary = (y_pred > 0.5).astype(np.int32)  # Convert probabilities to binary predictions\n",
    "\n",
    "# Calculate precision and recall\n",
    "precision = precision_score(y_test, y_pred_binary)\n",
    "recall = recall_score(y_test, y_pred_binary)\n",
    "accuracy = accuracy_score(y_test, y_pred_binary)\n",
    "f1 = f1_score(y_test, y_pred_binary)\n",
    "\n",
    "# Print precision and recall accuracy f1 score\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-Score:\", f1)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_binary)\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "cm = (np.array(cm))\n",
    "print(cm)\n",
    "\n",
    "print(f\"True Positives (TP): {TP}\")\n",
    "print(f\"True Negatives (TN): {TN}\")\n",
    "print(f\"False Positives (FP): {FP}\")\n",
    "print(f\"False Negatives (FN): {FN}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.2) Valutazione rete con fold cross validation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2.1) Visualizzazione media valori\n",
    "Per ottenere dei valori onesti delle metriche di valutazione di una rete neurale citate sopra, è opportuno calcolarne la media dei valori di tutti e 5 i modelli."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Precision:  0.9945775729646698\n",
      "Average Recall:  0.9780364455121433\n",
      "Average Accuracy:  0.9894606565919751\n",
      "Average F1-Score:  0.9861790095890546\n",
      "Average Mean Squared Error:  0.010539343408025013\n"
     ]
    }
   ],
   "source": [
    "averagePrecision = 0\n",
    "averageRecall = 0\n",
    "averageAccuracy = 0\n",
    "averageF1 = 0\n",
    "averageMse = 0\n",
    "\n",
    "for i in range(totFold):\n",
    "    averagePrecision += precisions[i]\n",
    "    averageRecall += recalls[i]\n",
    "    averageAccuracy += accuracies[i]\n",
    "    averageF1 += f1_scores[i]\n",
    "    averageMse += mse_scores[i]\n",
    "\n",
    "averagePrecision /= totFold\n",
    "averageRecall /= totFold\n",
    "averageAccuracy /= totFold\n",
    "averageF1 /= totFold\n",
    "averageMse /= totFold\n",
    "\n",
    "print('Average Precision: ', averagePrecision)\n",
    "print('Average Recall: ',averageRecall)\n",
    "print('Average Accuracy: ', averageAccuracy)\n",
    "print('Average F1-Score: ', averageF1)\n",
    "print('Average Mean Squared Error: ', averageMse)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T20:15:02.507501300Z",
     "start_time": "2023-09-07T20:15:02.498346600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5.2.2) Visualizzazione valori di tutti i modelli\n",
    "In questo blocco di codice, vengono mostrati i valori delle metriche di valutazione e la confusion matrix per ogni modello addestrato precedentemente. Alla fine, bisogna sceglierne uno."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Fold\n",
      "Precision:  1.0\n",
      "Recall:  0.975\n",
      "Accuracy:  0.9901315789473685\n",
      "F1-Score:  0.9873417721518987\n",
      "Mean Squared Error:  0.009868421052631578\n",
      "\n",
      "True Positives (TP): 117\n",
      "True Negatives (TN): 184\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 3\n",
      "\n",
      "2) Fold\n",
      "Precision:  1.0\n",
      "Recall:  1.0\n",
      "Accuracy:  1.0\n",
      "F1-Score:  1.0\n",
      "Mean Squared Error:  0.0\n",
      "\n",
      "True Positives (TP): 118\n",
      "True Negatives (TN): 186\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 0\n",
      "\n",
      "3) Fold\n",
      "Precision:  0.9919354838709677\n",
      "Recall:  0.9919354838709677\n",
      "Accuracy:  0.993421052631579\n",
      "F1-Score:  0.9919354838709677\n",
      "Mean Squared Error:  0.006578947368421052\n",
      "\n",
      "True Positives (TP): 123\n",
      "True Negatives (TN): 179\n",
      "False Positives (FP): 1\n",
      "False Negatives (FN): 1\n",
      "\n",
      "4) Fold\n",
      "Precision:  1.0\n",
      "Recall:  0.9606299212598425\n",
      "Accuracy:  0.9835526315789473\n",
      "F1-Score:  0.9799196787148594\n",
      "Mean Squared Error:  0.01644736842105263\n",
      "\n",
      "True Positives (TP): 122\n",
      "True Negatives (TN): 177\n",
      "False Positives (FP): 0\n",
      "False Negatives (FN): 5\n",
      "\n",
      "5) Fold\n",
      "Precision:  0.9809523809523809\n",
      "Recall:  0.9626168224299065\n",
      "Accuracy:  0.9801980198019802\n",
      "F1-Score:  0.9716981132075471\n",
      "Mean Squared Error:  0.019801980198019802\n",
      "\n",
      "True Positives (TP): 103\n",
      "True Negatives (TN): 194\n",
      "False Positives (FP): 2\n",
      "False Negatives (FN): 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(totFold):\n",
    "    print(f'{i+1}) Fold')\n",
    "    print('Precision: ', precisions[i])\n",
    "    print('Recall: ', recalls[i])\n",
    "    print('Accuracy: ', accuracies[i])\n",
    "    print('F1-Score: ', f1_scores[i])\n",
    "    print('Mean Squared Error: ', mse_scores[i])\n",
    "\n",
    "    TN, FP, FN, TP = cms[i].ravel()\n",
    "    print()\n",
    "    print(f\"True Positives (TP): {TP}\")\n",
    "    print(f\"True Negatives (TN): {TN}\")\n",
    "    print(f\"False Positives (FP): {FP}\")\n",
    "    print(f\"False Negatives (FN): {FN}\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T20:16:01.534081600Z",
     "start_time": "2023-09-07T20:16:01.510733500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 5.3) Visualizzazione grafico addestramento\n",
    "Se viene utilizzato il set di validation per l'addestramento, rimuovere i commenti per la visualizzazione di \"val_loss\" e \"val_accuracy\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#plotting results\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "#plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=15)\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "#plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=15)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 6) Salvataggio modello\n",
    "In questo modo è possibile salvare il modello in memoria"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save(os.path.join('models','rete_1.h5'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Salvataggio di tutti i modelli ottenuti dalla Fold Cross-Validation\n",
    "for i in range(len(models)):\n",
    "    models[i].save(os.path.join('models',f'{i+1}Fold_5.h5'))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 7) Caricamento modello\n",
    "Per utilizzare un modello vecchio utilizzato in precedenza, non è necessario rifare l'addestramento, ma basta semplicemente caricarlo"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "model = saving.load_model('models\\\\__rete5_d0_tilde.h5')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T16:57:43.525251800Z",
     "start_time": "2023-09-10T16:57:43.335207100Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 8) Processamento audio\n",
    "A questo punto, si vuole far lavorare la rete su un audio. La sequenza è questa:\n",
    "\n",
    "1) Caricamento audio e creazione degli spettrogrammi in scala di grigio\n",
    "2) Filtraggio immagini con filtro Sobel, implementato nel file ImageFilter.py\n",
    "3) Caricamento immagini in due dimensioni, ridimensionate (224, 224) e raggruppate in batch\n",
    "4) Previsione da parte della rete\n",
    "\n",
    "Alla fine si ottiene un array di percentuali, ogni valore si riferisce ad un immagine."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.1) Definizione funzioni"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "outputs": [],
   "source": [
    "#Questa funzione prende in ingresso un file wav e ne crea gli spettrogrammi in scala di grigio.\n",
    "#Vengono selezionate solo le frequenze tra 5 kHz e 25 kHz.\n",
    "#Le immagini fanno riferimento a frame di 600 ms, che scorrono di 200 ms. Per fare questa suddivisione temporale, viene creato un vettore che fa riferimento al tempo. In un ciclo vengono selezionati gli intervalli opportuni e viene memorizzata l'immagine in scala di grigio.\n",
    "#I nomi dei file che vengono creati sono valori numerici in ordine crescente.\n",
    "# Facendo questa suddivisione temporale, per una registrazione di un'ora si dovrebbero avere 17988 immagini, per una registrazione di 5 minuti 1498 immagini.\n",
    "\n",
    "def getAndSaveSpectrogram(filename, outputPath, window_size=2048, overlap=1024):\n",
    "    fs, signal = sci.io.wavfile.read(filename)\n",
    "    step = window_size - overlap\n",
    "    bins = np.arange(0, len(signal) - window_size + 1, step)\n",
    "    window = np.hanning(window_size)\n",
    "    spectrogram = []\n",
    "\n",
    "    n_time_steps = int((len(signal) - window_size) / overlap) + 1\n",
    "\n",
    "    time = ((n_time_steps + 1) * overlap) / fs\n",
    "    print(f'{time} secondi')\n",
    "    #print(str(n_time_steps) + ' time step')\n",
    "\n",
    "    for start in bins:\n",
    "        segment = signal[start:start + window_size] * window\n",
    "        spectrum = np.fft.fft(segment, n=2048)[:window_size // 2]\n",
    "        magnitude = np.abs(spectrum)\n",
    "        magnitude_db = 20 * np.log10(magnitude)\n",
    "        spectrogram.append(magnitude_db)\n",
    "\n",
    "    spectrogram = np.array(spectrogram).T\n",
    "\n",
    "    max_freq = 25000\n",
    "    freq_bins = int(max_freq / (fs / window_size))\n",
    "    min_freq = 5000\n",
    "    freq_bins_min = int(min_freq / (fs / window_size))\n",
    "\n",
    "    #375 step corrispondono ad un secondo\n",
    "    oneSecStep = (n_time_steps / time) + 1\n",
    "    lengthFrame = oneSecStep * 0.6\n",
    "    print(oneSecStep, lengthFrame, oneSecStep * 0.2)\n",
    "    lengthStep = 0.2\n",
    "    n_frame = int(time / lengthStep) - 2\n",
    "\n",
    "    arrayTime = [0]\n",
    "    toNext = float((n_time_steps) / (n_frame+2))\n",
    "\n",
    "    for k in range (1, n_frame+2):\n",
    "        number = arrayTime[k-1] + toNext + (k%2)\n",
    "        arrayTime.append(int(number))\n",
    "\n",
    "    #print(arrayTime)\n",
    "\n",
    "    for i in range(n_frame - 1):\n",
    "        print(i + 1, n_frame)\n",
    "        frame = spectrogram[freq_bins_min:freq_bins, arrayTime[i]:arrayTime[i + 3]]\n",
    "        plt.gray()\n",
    "        plt.rcParams['axes.grid'] = False\n",
    "        plt.rcParams['image.origin'] = 'lower'\n",
    "        plt.rcParams['image.aspect'] = 'auto'\n",
    "        plt.axis('off')\n",
    "\n",
    "        plt.imshow(frame)\n",
    "        outputFile = f'{outputPath}\\\\{i+1}.png'\n",
    "        plt.savefig(outputFile, bbox_inches='tight', pad_inches=0)\n",
    "        plt.close()\n",
    "\n",
    "    frame = spectrogram[freq_bins_min:freq_bins, arrayTime[n_frame - 1]:n_time_steps - 1]\n",
    "    plt.gray()\n",
    "    plt.rcParams['axes.grid'] = False\n",
    "    plt.rcParams['image.origin'] = 'lower'\n",
    "    plt.rcParams['image.aspect'] = 'auto'\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.imshow(frame)\n",
    "    outputFile = f'{outputPath}\\\\{n_frame}.png'\n",
    "    if n_frame==0:\n",
    "        outputFile = f'{outputPath}\\\\1.png'\n",
    "\n",
    "    plt.savefig(outputFile, bbox_inches='tight', pad_inches=0)\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "#Questa funzione serve per eliminare tutti i file all'interno di una cartella\n",
    "def delete_all_files_in_directory(directory_path):\n",
    "    for filename in os.listdir(directory_path):\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        try:\n",
    "            if os.path.isfile(file_path):\n",
    "                os.unlink(file_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error deleting file {file_path}. Reason: {e}\")\n",
    "\n",
    "\n",
    "#Una volta creati gli spettrogrammi, questa funzione può essere utilizzata per far predirre alla rete le immagini.\n",
    "#Se devono ancora essere filtrate tramite filtro Sobel, settare a True il parametro \"toFilter\". Se si hanno già gli spettrogrammi filtrati, è sufficiente richiamare questa funzione con il parametro toFilter settato a False. Questo permette di non dover creare ogni volta gli spettrogrammi se si vuole lavorare più volte con la stessa registrazione.\n",
    "#Le immagini vengono raggruppate in batch, perchè in questo modo la rete è più veloce piuttosto che analizzare le immagini una ad una.\n",
    "#Dopodichè, viene utilizzata la rete neurale per effettuare la previsione delle immagini.\n",
    "#Questa funzione restituisce il risultato della previsione, ovvero un vettore di percentuali\n",
    "\n",
    "def predictOnSpectrogramsBatches(pathSpectrograms, model, toFilter=False):\n",
    "\n",
    "    if toFilter==True:\n",
    "        filter.sobelFilterDirectory(pathSpectrograms, pathSpectrograms)\n",
    "\n",
    "    spectrogramList = []\n",
    "    spectrogram_list_os = os.listdir(pathSpectrograms)\n",
    "\n",
    "    for i in range(len(spectrogram_list_os)):\n",
    "        filePath = os.path.join(pathSpectrograms, f'{i + 1}.png')\n",
    "        image = Image.open(filePath).convert('L')  # Convert to grayscale\n",
    "        image = image.resize((224, 224))  # Resize if needed\n",
    "        image = np.array(image).transpose()  # Convert to NumPy array\n",
    "        spectrogramList.append((np.expand_dims(image, 0)).transpose())\n",
    "        print(image.shape)\n",
    "\n",
    "    spectrogramList = np.array(spectrogramList)/255.0\n",
    "\n",
    "    BATCH_SIZE = 128\n",
    "    indices = list(range(BATCH_SIZE, len(spectrogramList), BATCH_SIZE))\n",
    "\n",
    "    # Split the array\n",
    "    split_arrays = np.split(spectrogramList, indices)\n",
    "\n",
    "    result = []\n",
    "    count = 0\n",
    "    # Display the split arrays\n",
    "    for sub_array in split_arrays:\n",
    "        yApp = model.predict(sub_array)\n",
    "        for k in range(len(yApp)):  #extend\n",
    "            result.append(yApp[k])\n",
    "        count += len(yApp)\n",
    "        print(str(count), 'done')\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "#Funzione utilizzata per effettuare tutti i passaggi descritti nell'elenco numerato. Gli spettrogrammi vengono memorizzati tutti all'interno della cartella \"Analize\"\n",
    "def predictWave(filename, model):\n",
    "    pathSpectrograms = 'Analize'\n",
    "    delete_all_files_in_directory(pathSpectrograms)\n",
    "    getAndSaveSpectrogram(filename, pathSpectrograms)\n",
    "    return predictOnSpectrogramsBatches(pathSpectrograms, model, True)\n",
    "\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T20:23:33.398149Z",
     "start_time": "2023-09-10T20:23:33.353452300Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.2) Avvio previsione\n",
    "Nel caso in cui gli spettrogrammi sono stati già fatti, eseguire \"predictOnSpectrogramsBatches\" invece di \"predictWave\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filePath = 'RegFatte\\\\20211120_174159_192.wav'\n",
    "result = predictWave(filePath, model)\n",
    "#result = predictOnSpectrogramsBatches('Analize', model)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 8.3) Visualizzazione risultati\n",
    "In questo blocco di codice vengono visualizzate le percentuali di appartenenza alla classe dei delfini.\n",
    "Vengono riportati sia il numero dell'immagine a cui si fa riferimento che l'istante temporale di partenza."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0:00:00 - prob: 0.0\n",
      "2 0:00:20 - prob: 0.0\n",
      "3 0:00:40 - prob: 0.0\n",
      "4 0:00:60 - prob: 0.0\n",
      "5 0:00:80 - prob: 0.0\n",
      "6 0:01:00 - prob: 0.0\n",
      "7 0:01:20 - prob: 0.0\n",
      "8 0:01:40 - prob: 0.0\n",
      "9 0:01:60 - prob: 0.0\n",
      "10 0:01:80 - prob: 0.0\n",
      "11 0:02:00 - prob: 0.0\n",
      "12 0:02:20 - prob: 0.0\n",
      "13 0:02:40 - prob: 0.0\n",
      "14 0:02:60 - prob: 0.0\n",
      "15 0:02:80 - prob: 100.0\n",
      "16 0:03:00 - prob: 100.0\n",
      "17 0:03:20 - prob: 100.0\n",
      "18 0:03:40 - prob: 99.9\n",
      "19 0:03:60 - prob: 0.0\n",
      "20 0:03:80 - prob: 0.0\n",
      "21 0:04:00 - prob: 0.0\n",
      "22 0:04:20 - prob: 0.0\n",
      "23 0:04:40 - prob: 0.0\n",
      "24 0:04:60 - prob: 0.0\n",
      "25 0:04:80 - prob: 0.0\n",
      "26 0:05:00 - prob: 0.0\n",
      "27 0:05:20 - prob: 0.0\n",
      "28 0:05:40 - prob: 0.0\n",
      "29 0:05:60 - prob: 0.0\n",
      "30 0:05:80 - prob: 21.2\n",
      "31 0:06:00 - prob: 0.0\n",
      "32 0:06:20 - prob: 0.0\n",
      "33 0:06:40 - prob: 0.0\n",
      "34 0:06:60 - prob: 0.0\n",
      "35 0:06:80 - prob: 0.0\n",
      "36 0:07:00 - prob: 0.0\n",
      "37 0:07:20 - prob: 0.0\n",
      "38 0:07:40 - prob: 0.0\n",
      "39 0:07:60 - prob: 0.0\n",
      "40 0:07:80 - prob: 0.0\n",
      "41 0:08:00 - prob: 0.0\n",
      "42 0:08:20 - prob: 0.0\n",
      "43 0:08:40 - prob: 0.0\n",
      "44 0:08:60 - prob: 0.0\n",
      "45 0:08:80 - prob: 0.0\n",
      "46 0:09:00 - prob: 0.0\n",
      "47 0:09:20 - prob: 0.0\n",
      "48 0:09:40 - prob: 0.0\n",
      "49 0:09:60 - prob: 0.0\n",
      "50 0:09:80 - prob: 0.0\n",
      "51 0:10:00 - prob: 0.0\n",
      "52 0:10:20 - prob: 0.0\n",
      "53 0:10:40 - prob: 0.0\n",
      "54 0:10:60 - prob: 0.0\n",
      "55 0:10:80 - prob: 0.0\n",
      "56 0:11:00 - prob: 0.0\n",
      "57 0:11:20 - prob: 0.0\n",
      "58 0:11:40 - prob: 0.0\n",
      "59 0:11:60 - prob: 0.0\n",
      "60 0:11:80 - prob: 0.1\n",
      "61 0:12:00 - prob: 0.0\n",
      "62 0:12:20 - prob: 0.0\n",
      "63 0:12:40 - prob: 0.0\n",
      "64 0:12:60 - prob: 0.0\n",
      "65 0:12:80 - prob: 0.0\n",
      "66 0:13:00 - prob: 0.0\n",
      "67 0:13:20 - prob: 76.5\n",
      "68 0:13:40 - prob: 100.0\n",
      "69 0:13:60 - prob: 100.0\n",
      "70 0:13:80 - prob: 99.9\n",
      "71 0:14:00 - prob: 20.2\n",
      "72 0:14:20 - prob: 3.2\n",
      "73 0:14:40 - prob: 0.0\n",
      "74 0:14:60 - prob: 0.2\n",
      "75 0:14:80 - prob: 2.8\n",
      "76 0:15:00 - prob: 0.3\n",
      "77 0:15:20 - prob: 0.3\n",
      "78 0:15:40 - prob: 0.0\n",
      "79 0:15:60 - prob: 0.0\n",
      "80 0:15:80 - prob: 0.0\n",
      "81 0:16:00 - prob: 0.0\n",
      "82 0:16:20 - prob: 0.0\n",
      "83 0:16:40 - prob: 0.0\n",
      "84 0:16:60 - prob: 17.1\n",
      "85 0:16:80 - prob: 3.4\n",
      "86 0:17:00 - prob: 4.2\n",
      "87 0:17:20 - prob: 0.0\n",
      "88 0:17:40 - prob: 0.0\n",
      "89 0:17:60 - prob: 2.6\n",
      "90 0:17:80 - prob: 0.5\n",
      "91 0:18:00 - prob: 0.0\n",
      "92 0:18:20 - prob: 0.0\n",
      "93 0:18:40 - prob: 0.0\n",
      "94 0:18:60 - prob: 0.9\n",
      "95 0:18:80 - prob: 8.4\n",
      "96 0:19:00 - prob: 0.0\n",
      "97 0:19:20 - prob: 0.0\n",
      "98 0:19:40 - prob: 0.0\n",
      "99 0:19:60 - prob: 99.9\n",
      "100 0:19:80 - prob: 100.0\n",
      "101 0:20:00 - prob: 100.0\n",
      "102 0:20:20 - prob: 0.9\n",
      "103 0:20:40 - prob: 7.0\n",
      "104 0:20:60 - prob: 0.1\n",
      "105 0:20:80 - prob: 0.0\n",
      "106 0:21:00 - prob: 0.0\n",
      "107 0:21:20 - prob: 0.0\n",
      "108 0:21:40 - prob: 0.0\n",
      "109 0:21:60 - prob: 100.0\n",
      "110 0:21:80 - prob: 100.0\n",
      "111 0:22:00 - prob: 100.0\n",
      "112 0:22:20 - prob: 0.0\n",
      "113 0:22:40 - prob: 0.0\n",
      "114 0:22:60 - prob: 0.0\n",
      "115 0:22:80 - prob: 0.0\n",
      "116 0:23:00 - prob: 0.0\n",
      "117 0:23:20 - prob: 0.0\n",
      "118 0:23:40 - prob: 0.0\n",
      "119 0:23:60 - prob: 0.0\n",
      "120 0:23:80 - prob: 0.0\n",
      "121 0:24:00 - prob: 0.0\n",
      "122 0:24:20 - prob: 0.0\n",
      "123 0:24:40 - prob: 0.1\n",
      "124 0:24:60 - prob: 0.0\n",
      "125 0:24:80 - prob: 0.0\n",
      "126 0:25:00 - prob: 0.0\n",
      "127 0:25:20 - prob: 0.0\n",
      "128 0:25:40 - prob: 0.0\n",
      "129 0:25:60 - prob: 0.0\n",
      "130 0:25:80 - prob: 0.0\n",
      "131 0:26:00 - prob: 0.0\n",
      "132 0:26:20 - prob: 0.0\n",
      "133 0:26:40 - prob: 0.0\n",
      "134 0:26:60 - prob: 0.0\n",
      "135 0:26:80 - prob: 57.6\n",
      "136 0:27:00 - prob: 16.5\n",
      "137 0:27:20 - prob: 0.0\n",
      "138 0:27:40 - prob: 0.0\n",
      "139 0:27:60 - prob: 5.4\n",
      "140 0:27:80 - prob: 1.9\n",
      "141 0:28:00 - prob: 0.0\n",
      "142 0:28:20 - prob: 0.0\n",
      "143 0:28:40 - prob: 0.0\n",
      "144 0:28:60 - prob: 0.0\n",
      "145 0:28:80 - prob: 0.0\n",
      "146 0:29:00 - prob: 0.0\n",
      "147 0:29:20 - prob: 0.0\n",
      "148 0:29:40 - prob: 0.2\n",
      "149 0:29:60 - prob: 1.0\n",
      "150 0:29:80 - prob: 0.0\n",
      "151 0:30:00 - prob: 0.0\n",
      "152 0:30:20 - prob: 0.0\n",
      "153 0:30:40 - prob: 0.0\n",
      "154 0:30:60 - prob: 0.0\n",
      "155 0:30:80 - prob: 0.0\n",
      "156 0:31:00 - prob: 0.0\n",
      "157 0:31:20 - prob: 0.0\n",
      "158 0:31:40 - prob: 0.0\n",
      "159 0:31:60 - prob: 0.0\n",
      "160 0:31:80 - prob: 0.0\n",
      "161 0:32:00 - prob: 0.0\n",
      "162 0:32:20 - prob: 0.0\n",
      "163 0:32:40 - prob: 0.0\n",
      "164 0:32:60 - prob: 0.0\n",
      "165 0:32:80 - prob: 0.0\n",
      "166 0:33:00 - prob: 0.0\n",
      "167 0:33:20 - prob: 0.0\n",
      "168 0:33:40 - prob: 0.3\n",
      "169 0:33:60 - prob: 0.1\n",
      "170 0:33:80 - prob: 0.0\n",
      "171 0:34:00 - prob: 0.0\n",
      "172 0:34:20 - prob: 0.0\n",
      "173 0:34:40 - prob: 0.0\n",
      "174 0:34:60 - prob: 0.0\n",
      "175 0:34:80 - prob: 0.0\n",
      "176 0:35:00 - prob: 0.4\n",
      "177 0:35:20 - prob: 0.0\n",
      "178 0:35:40 - prob: 99.9\n",
      "179 0:35:60 - prob: 100.0\n",
      "180 0:35:80 - prob: 100.0\n",
      "181 0:36:00 - prob: 99.9\n",
      "182 0:36:20 - prob: 3.9\n",
      "183 0:36:40 - prob: 0.0\n",
      "184 0:36:60 - prob: 0.0\n",
      "185 0:36:80 - prob: 0.0\n",
      "186 0:37:00 - prob: 0.0\n",
      "187 0:37:20 - prob: 0.0\n",
      "188 0:37:40 - prob: 0.0\n",
      "189 0:37:60 - prob: 0.0\n",
      "190 0:37:80 - prob: 0.0\n",
      "191 0:38:00 - prob: 0.0\n",
      "192 0:38:20 - prob: 0.0\n",
      "193 0:38:40 - prob: 0.0\n",
      "194 0:38:60 - prob: 0.0\n",
      "195 0:38:80 - prob: 0.0\n",
      "196 0:39:00 - prob: 0.0\n",
      "197 0:39:20 - prob: 0.1\n",
      "198 0:39:40 - prob: 0.1\n",
      "199 0:39:60 - prob: 2.9\n",
      "200 0:39:80 - prob: 0.0\n",
      "201 0:40:00 - prob: 0.0\n",
      "202 0:40:20 - prob: 3.3\n",
      "203 0:40:40 - prob: 0.0\n",
      "204 0:40:60 - prob: 0.0\n",
      "205 0:40:80 - prob: 0.0\n",
      "206 0:41:00 - prob: 0.0\n",
      "207 0:41:20 - prob: 0.0\n",
      "208 0:41:40 - prob: 0.1\n",
      "209 0:41:60 - prob: 0.0\n",
      "210 0:41:80 - prob: 0.0\n",
      "211 0:42:00 - prob: 0.0\n",
      "212 0:42:20 - prob: 0.0\n",
      "213 0:42:40 - prob: 0.2\n",
      "214 0:42:60 - prob: 0.0\n",
      "215 0:42:80 - prob: 0.1\n",
      "216 0:43:00 - prob: 0.0\n",
      "217 0:43:20 - prob: 0.0\n",
      "218 0:43:40 - prob: 0.0\n",
      "219 0:43:60 - prob: 0.0\n",
      "220 0:43:80 - prob: 0.0\n",
      "221 0:44:00 - prob: 0.0\n",
      "222 0:44:20 - prob: 0.0\n",
      "223 0:44:40 - prob: 0.3\n",
      "224 0:44:60 - prob: 0.1\n",
      "225 0:44:80 - prob: 0.0\n",
      "226 0:45:00 - prob: 0.0\n",
      "227 0:45:20 - prob: 0.0\n",
      "228 0:45:40 - prob: 0.0\n",
      "229 0:45:60 - prob: 0.0\n",
      "230 0:45:80 - prob: 0.0\n",
      "231 0:46:00 - prob: 0.0\n",
      "232 0:46:20 - prob: 0.0\n",
      "233 0:46:40 - prob: 0.0\n",
      "234 0:46:60 - prob: 0.0\n",
      "235 0:46:80 - prob: 0.0\n",
      "236 0:47:00 - prob: 0.0\n",
      "237 0:47:20 - prob: 0.2\n",
      "238 0:47:40 - prob: 0.0\n",
      "239 0:47:60 - prob: 0.0\n",
      "240 0:47:80 - prob: 0.0\n",
      "241 0:48:00 - prob: 0.0\n",
      "242 0:48:20 - prob: 0.0\n",
      "243 0:48:40 - prob: 0.0\n",
      "244 0:48:60 - prob: 0.0\n",
      "245 0:48:80 - prob: 0.0\n",
      "246 0:49:00 - prob: 0.0\n",
      "247 0:49:20 - prob: 0.0\n",
      "248 0:49:40 - prob: 0.0\n",
      "249 0:49:60 - prob: 0.0\n",
      "250 0:49:80 - prob: 0.0\n",
      "251 0:50:00 - prob: 0.0\n",
      "252 0:50:20 - prob: 0.0\n",
      "253 0:50:40 - prob: 0.0\n",
      "254 0:50:60 - prob: 0.0\n",
      "255 0:50:80 - prob: 0.0\n",
      "256 0:51:00 - prob: 0.0\n",
      "257 0:51:20 - prob: 0.0\n",
      "258 0:51:40 - prob: 0.0\n",
      "259 0:51:60 - prob: 0.5\n",
      "260 0:51:80 - prob: 0.0\n",
      "261 0:52:00 - prob: 0.0\n",
      "262 0:52:20 - prob: 0.0\n",
      "263 0:52:40 - prob: 0.0\n",
      "264 0:52:60 - prob: 0.0\n",
      "265 0:52:80 - prob: 0.0\n",
      "266 0:53:00 - prob: 0.0\n",
      "267 0:53:20 - prob: 0.0\n",
      "268 0:53:40 - prob: 0.0\n",
      "269 0:53:60 - prob: 0.0\n",
      "270 0:53:80 - prob: 0.0\n",
      "271 0:54:00 - prob: 0.0\n",
      "272 0:54:20 - prob: 0.0\n",
      "273 0:54:40 - prob: 0.0\n",
      "274 0:54:60 - prob: 1.9\n",
      "275 0:54:80 - prob: 0.0\n",
      "276 0:55:00 - prob: 0.0\n",
      "277 0:55:20 - prob: 0.0\n",
      "278 0:55:40 - prob: 0.0\n",
      "279 0:55:60 - prob: 0.0\n",
      "280 0:55:80 - prob: 0.0\n",
      "281 0:56:00 - prob: 0.5\n",
      "282 0:56:20 - prob: 1.5\n",
      "283 0:56:40 - prob: 0.0\n",
      "284 0:56:60 - prob: 0.0\n",
      "285 0:56:80 - prob: 0.0\n",
      "286 0:57:00 - prob: 0.0\n",
      "287 0:57:20 - prob: 0.0\n",
      "288 0:57:40 - prob: 0.0\n",
      "289 0:57:60 - prob: 0.0\n",
      "290 0:57:80 - prob: 0.0\n",
      "291 0:58:00 - prob: 0.0\n",
      "292 0:58:20 - prob: 0.0\n",
      "293 0:58:40 - prob: 0.0\n",
      "294 0:58:60 - prob: 0.0\n",
      "295 0:58:80 - prob: 0.0\n",
      "296 0:59:00 - prob: 0.0\n",
      "297 0:59:20 - prob: 0.0\n",
      "298 0:59:40 - prob: 0.0\n",
      "299 0:59:60 - prob: 0.0\n",
      "300 0:59:80 - prob: 0.1\n",
      "301 1:00:00 - prob: 0.1\n",
      "302 1:00:20 - prob: 0.0\n",
      "303 1:00:40 - prob: 0.8\n",
      "304 1:00:60 - prob: 36.7\n",
      "305 1:00:80 - prob: 0.4\n",
      "306 1:01:00 - prob: 0.0\n",
      "307 1:01:20 - prob: 0.2\n",
      "308 1:01:40 - prob: 0.0\n",
      "309 1:01:60 - prob: 0.0\n",
      "310 1:01:80 - prob: 0.0\n",
      "311 1:02:00 - prob: 0.0\n",
      "312 1:02:20 - prob: 0.0\n",
      "313 1:02:40 - prob: 0.0\n",
      "314 1:02:60 - prob: 0.0\n",
      "315 1:02:80 - prob: 0.0\n",
      "316 1:03:00 - prob: 0.0\n",
      "317 1:03:20 - prob: 0.0\n",
      "318 1:03:40 - prob: 80.5\n",
      "319 1:03:60 - prob: 99.9\n",
      "320 1:03:80 - prob: 100.0\n",
      "321 1:04:00 - prob: 100.0\n",
      "322 1:04:20 - prob: 99.9\n",
      "323 1:04:40 - prob: 0.0\n",
      "324 1:04:60 - prob: 0.0\n",
      "325 1:04:80 - prob: 0.0\n",
      "326 1:05:00 - prob: 0.0\n",
      "327 1:05:20 - prob: 0.0\n",
      "328 1:05:40 - prob: 0.0\n",
      "329 1:05:60 - prob: 0.0\n",
      "330 1:05:80 - prob: 0.0\n",
      "331 1:06:00 - prob: 3.2\n",
      "332 1:06:20 - prob: 68.9\n",
      "333 1:06:40 - prob: 0.0\n",
      "334 1:06:60 - prob: 0.3\n",
      "335 1:06:80 - prob: 0.0\n",
      "336 1:07:00 - prob: 0.1\n",
      "337 1:07:20 - prob: 0.0\n",
      "338 1:07:40 - prob: 0.0\n",
      "339 1:07:60 - prob: 0.0\n",
      "340 1:07:80 - prob: 0.0\n",
      "341 1:08:00 - prob: 0.0\n",
      "342 1:08:20 - prob: 0.0\n",
      "343 1:08:40 - prob: 0.0\n",
      "344 1:08:60 - prob: 95.6\n",
      "345 1:08:80 - prob: 99.9\n",
      "346 1:09:00 - prob: 99.9\n",
      "347 1:09:20 - prob: 0.0\n",
      "348 1:09:40 - prob: 0.0\n",
      "349 1:09:60 - prob: 0.0\n",
      "350 1:09:80 - prob: 0.0\n",
      "351 1:10:00 - prob: 0.0\n",
      "352 1:10:20 - prob: 0.0\n",
      "353 1:10:40 - prob: 0.0\n",
      "354 1:10:60 - prob: 0.0\n",
      "355 1:10:80 - prob: 0.0\n",
      "356 1:11:00 - prob: 0.0\n",
      "357 1:11:20 - prob: 0.0\n",
      "358 1:11:40 - prob: 0.0\n",
      "359 1:11:60 - prob: 0.0\n",
      "360 1:11:80 - prob: 0.0\n",
      "361 1:12:00 - prob: 0.0\n",
      "362 1:12:20 - prob: 0.0\n",
      "363 1:12:40 - prob: 0.1\n",
      "364 1:12:60 - prob: 15.2\n",
      "365 1:12:80 - prob: 14.9\n",
      "366 1:13:00 - prob: 0.0\n",
      "367 1:13:20 - prob: 0.0\n",
      "368 1:13:40 - prob: 0.0\n",
      "369 1:13:60 - prob: 0.0\n",
      "370 1:13:80 - prob: 0.8\n",
      "371 1:14:00 - prob: 2.9\n",
      "372 1:14:20 - prob: 0.8\n",
      "373 1:14:40 - prob: 46.8\n",
      "374 1:14:60 - prob: 100.0\n",
      "375 1:14:80 - prob: 100.0\n",
      "376 1:15:00 - prob: 0.4\n",
      "377 1:15:20 - prob: 0.0\n",
      "378 1:15:40 - prob: 0.0\n",
      "379 1:15:60 - prob: 0.0\n",
      "380 1:15:80 - prob: 0.1\n",
      "381 1:16:00 - prob: 0.0\n",
      "382 1:16:20 - prob: 0.1\n",
      "383 1:16:40 - prob: 0.4\n",
      "384 1:16:60 - prob: 0.6\n",
      "385 1:16:80 - prob: 0.0\n",
      "386 1:17:00 - prob: 0.0\n",
      "387 1:17:20 - prob: 0.0\n",
      "388 1:17:40 - prob: 0.1\n",
      "389 1:17:60 - prob: 0.0\n",
      "390 1:17:80 - prob: 0.0\n",
      "391 1:18:00 - prob: 0.0\n",
      "392 1:18:20 - prob: 0.4\n",
      "393 1:18:40 - prob: 0.1\n",
      "394 1:18:60 - prob: 0.4\n",
      "395 1:18:80 - prob: 0.0\n",
      "396 1:19:00 - prob: 0.0\n",
      "397 1:19:20 - prob: 0.0\n",
      "398 1:19:40 - prob: 0.0\n",
      "399 1:19:60 - prob: 0.0\n",
      "400 1:19:80 - prob: 0.0\n",
      "401 1:20:00 - prob: 0.0\n",
      "402 1:20:20 - prob: 0.0\n",
      "403 1:20:40 - prob: 0.0\n",
      "404 1:20:60 - prob: 0.0\n",
      "405 1:20:80 - prob: 0.0\n",
      "406 1:21:00 - prob: 0.0\n",
      "407 1:21:20 - prob: 0.0\n",
      "408 1:21:40 - prob: 0.0\n",
      "409 1:21:60 - prob: 0.1\n",
      "410 1:21:80 - prob: 0.1\n",
      "411 1:22:00 - prob: 0.0\n",
      "412 1:22:20 - prob: 0.0\n",
      "413 1:22:40 - prob: 0.0\n",
      "414 1:22:60 - prob: 0.0\n",
      "415 1:22:80 - prob: 0.1\n",
      "416 1:23:00 - prob: 0.0\n",
      "417 1:23:20 - prob: 0.0\n",
      "418 1:23:40 - prob: 0.0\n",
      "419 1:23:60 - prob: 0.0\n",
      "420 1:23:80 - prob: 0.0\n",
      "421 1:24:00 - prob: 0.0\n",
      "422 1:24:20 - prob: 0.0\n",
      "423 1:24:40 - prob: 0.0\n",
      "424 1:24:60 - prob: 0.0\n",
      "425 1:24:80 - prob: 0.0\n",
      "426 1:25:00 - prob: 0.0\n",
      "427 1:25:20 - prob: 0.1\n",
      "428 1:25:40 - prob: 0.1\n",
      "429 1:25:60 - prob: 0.5\n",
      "430 1:25:80 - prob: 0.1\n",
      "431 1:26:00 - prob: 0.4\n",
      "432 1:26:20 - prob: 0.0\n",
      "433 1:26:40 - prob: 12.9\n",
      "434 1:26:60 - prob: 0.0\n",
      "435 1:26:80 - prob: 0.0\n",
      "436 1:27:00 - prob: 0.0\n",
      "437 1:27:20 - prob: 0.0\n",
      "438 1:27:40 - prob: 0.0\n",
      "439 1:27:60 - prob: 0.0\n",
      "440 1:27:80 - prob: 0.0\n",
      "441 1:28:00 - prob: 0.0\n",
      "442 1:28:20 - prob: 0.0\n",
      "443 1:28:40 - prob: 0.0\n",
      "444 1:28:60 - prob: 0.1\n",
      "445 1:28:80 - prob: 0.7\n",
      "446 1:29:00 - prob: 0.3\n",
      "447 1:29:20 - prob: 0.0\n",
      "448 1:29:40 - prob: 0.0\n",
      "449 1:29:60 - prob: 0.0\n",
      "450 1:29:80 - prob: 0.0\n",
      "451 1:30:00 - prob: 0.0\n",
      "452 1:30:20 - prob: 0.0\n",
      "453 1:30:40 - prob: 0.0\n",
      "454 1:30:60 - prob: 0.0\n",
      "455 1:30:80 - prob: 0.0\n",
      "456 1:31:00 - prob: 0.0\n",
      "457 1:31:20 - prob: 0.0\n",
      "458 1:31:40 - prob: 0.0\n",
      "459 1:31:60 - prob: 0.0\n",
      "460 1:31:80 - prob: 0.0\n",
      "461 1:32:00 - prob: 0.0\n",
      "462 1:32:20 - prob: 0.0\n",
      "463 1:32:40 - prob: 0.0\n",
      "464 1:32:60 - prob: 0.0\n",
      "465 1:32:80 - prob: 0.0\n",
      "466 1:33:00 - prob: 0.0\n",
      "467 1:33:20 - prob: 0.0\n",
      "468 1:33:40 - prob: 0.2\n",
      "469 1:33:60 - prob: 0.0\n",
      "470 1:33:80 - prob: 0.0\n",
      "471 1:34:00 - prob: 0.0\n",
      "472 1:34:20 - prob: 0.0\n",
      "473 1:34:40 - prob: 0.0\n",
      "474 1:34:60 - prob: 0.0\n",
      "475 1:34:80 - prob: 0.0\n",
      "476 1:35:00 - prob: 0.0\n",
      "477 1:35:20 - prob: 0.0\n",
      "478 1:35:40 - prob: 0.0\n",
      "479 1:35:60 - prob: 0.0\n",
      "480 1:35:80 - prob: 0.0\n",
      "481 1:36:00 - prob: 0.0\n",
      "482 1:36:20 - prob: 0.0\n",
      "483 1:36:40 - prob: 0.0\n",
      "484 1:36:60 - prob: 0.0\n",
      "485 1:36:80 - prob: 0.0\n",
      "486 1:37:00 - prob: 0.0\n",
      "487 1:37:20 - prob: 0.0\n",
      "488 1:37:40 - prob: 0.0\n",
      "489 1:37:60 - prob: 0.0\n",
      "490 1:37:80 - prob: 0.0\n",
      "491 1:38:00 - prob: 0.0\n",
      "492 1:38:20 - prob: 0.0\n",
      "493 1:38:40 - prob: 0.1\n",
      "494 1:38:60 - prob: 0.1\n",
      "495 1:38:80 - prob: 0.1\n",
      "496 1:39:00 - prob: 0.4\n",
      "497 1:39:20 - prob: 0.5\n",
      "498 1:39:40 - prob: 0.4\n",
      "499 1:39:60 - prob: 0.0\n",
      "500 1:39:80 - prob: 0.0\n",
      "501 1:40:00 - prob: 0.0\n",
      "502 1:40:20 - prob: 0.0\n",
      "503 1:40:40 - prob: 0.0\n",
      "504 1:40:60 - prob: 0.2\n",
      "505 1:40:80 - prob: 0.0\n",
      "506 1:41:00 - prob: 0.0\n",
      "507 1:41:20 - prob: 0.3\n",
      "508 1:41:40 - prob: 1.0\n",
      "509 1:41:60 - prob: 0.0\n",
      "510 1:41:80 - prob: 0.0\n",
      "511 1:42:00 - prob: 0.0\n",
      "512 1:42:20 - prob: 0.1\n",
      "513 1:42:40 - prob: 0.7\n",
      "514 1:42:60 - prob: 0.0\n",
      "515 1:42:80 - prob: 0.0\n",
      "516 1:43:00 - prob: 0.0\n",
      "517 1:43:20 - prob: 0.0\n",
      "518 1:43:40 - prob: 0.0\n",
      "519 1:43:60 - prob: 0.0\n",
      "520 1:43:80 - prob: 0.0\n",
      "521 1:44:00 - prob: 0.0\n",
      "522 1:44:20 - prob: 0.0\n",
      "523 1:44:40 - prob: 0.0\n",
      "524 1:44:60 - prob: 0.0\n",
      "525 1:44:80 - prob: 0.0\n",
      "526 1:45:00 - prob: 0.0\n",
      "527 1:45:20 - prob: 0.0\n",
      "528 1:45:40 - prob: 0.0\n",
      "529 1:45:60 - prob: 0.0\n",
      "530 1:45:80 - prob: 0.0\n",
      "531 1:46:00 - prob: 0.0\n",
      "532 1:46:20 - prob: 0.2\n",
      "533 1:46:40 - prob: 0.0\n",
      "534 1:46:60 - prob: 0.0\n",
      "535 1:46:80 - prob: 0.0\n",
      "536 1:47:00 - prob: 0.0\n",
      "537 1:47:20 - prob: 0.0\n",
      "538 1:47:40 - prob: 0.0\n",
      "539 1:47:60 - prob: 0.0\n",
      "540 1:47:80 - prob: 0.0\n",
      "541 1:48:00 - prob: 0.0\n",
      "542 1:48:20 - prob: 0.0\n",
      "543 1:48:40 - prob: 0.0\n",
      "544 1:48:60 - prob: 0.0\n",
      "545 1:48:80 - prob: 0.0\n",
      "546 1:49:00 - prob: 0.0\n",
      "547 1:49:20 - prob: 0.0\n",
      "548 1:49:40 - prob: 0.0\n",
      "549 1:49:60 - prob: 0.0\n",
      "550 1:49:80 - prob: 0.0\n",
      "551 1:50:00 - prob: 0.0\n",
      "552 1:50:20 - prob: 0.0\n",
      "553 1:50:40 - prob: 0.0\n",
      "554 1:50:60 - prob: 0.0\n",
      "555 1:50:80 - prob: 0.0\n",
      "556 1:51:00 - prob: 0.0\n",
      "557 1:51:20 - prob: 0.0\n",
      "558 1:51:40 - prob: 0.0\n",
      "559 1:51:60 - prob: 0.0\n",
      "560 1:51:80 - prob: 0.0\n",
      "561 1:52:00 - prob: 0.0\n",
      "562 1:52:20 - prob: 0.0\n",
      "563 1:52:40 - prob: 0.0\n",
      "564 1:52:60 - prob: 0.1\n",
      "565 1:52:80 - prob: 0.1\n",
      "566 1:53:00 - prob: 0.0\n",
      "567 1:53:20 - prob: 0.0\n",
      "568 1:53:40 - prob: 1.7\n",
      "569 1:53:60 - prob: 0.1\n",
      "570 1:53:80 - prob: 0.1\n",
      "571 1:54:00 - prob: 0.0\n",
      "572 1:54:20 - prob: 1.0\n",
      "573 1:54:40 - prob: 0.1\n",
      "574 1:54:60 - prob: 0.0\n",
      "575 1:54:80 - prob: 0.0\n",
      "576 1:55:00 - prob: 0.0\n",
      "577 1:55:20 - prob: 0.0\n",
      "578 1:55:40 - prob: 0.0\n",
      "579 1:55:60 - prob: 0.0\n",
      "580 1:55:80 - prob: 0.0\n",
      "581 1:56:00 - prob: 0.0\n",
      "582 1:56:20 - prob: 0.0\n",
      "583 1:56:40 - prob: 0.0\n",
      "584 1:56:60 - prob: 0.0\n",
      "585 1:56:80 - prob: 0.0\n",
      "586 1:57:00 - prob: 0.0\n",
      "587 1:57:20 - prob: 0.0\n",
      "588 1:57:40 - prob: 0.0\n",
      "589 1:57:60 - prob: 0.0\n",
      "590 1:57:80 - prob: 0.1\n",
      "591 1:58:00 - prob: 0.0\n",
      "592 1:58:20 - prob: 0.0\n",
      "593 1:58:40 - prob: 0.0\n",
      "594 1:58:60 - prob: 0.0\n",
      "595 1:58:80 - prob: 0.0\n",
      "596 1:59:00 - prob: 6.4\n",
      "597 1:59:20 - prob: 1.8\n",
      "598 1:59:40 - prob: 0.0\n",
      "599 1:59:60 - prob: 0.0\n",
      "600 1:59:80 - prob: 0.0\n",
      "601 2:00:00 - prob: 0.0\n",
      "602 2:00:20 - prob: 0.0\n",
      "603 2:00:40 - prob: 0.0\n",
      "604 2:00:60 - prob: 0.0\n",
      "605 2:00:80 - prob: 0.0\n",
      "606 2:01:00 - prob: 0.0\n",
      "607 2:01:20 - prob: 0.0\n",
      "608 2:01:40 - prob: 0.0\n",
      "609 2:01:60 - prob: 0.0\n",
      "610 2:01:80 - prob: 0.0\n",
      "611 2:02:00 - prob: 0.0\n",
      "612 2:02:20 - prob: 0.0\n",
      "613 2:02:40 - prob: 0.0\n",
      "614 2:02:60 - prob: 0.0\n",
      "615 2:02:80 - prob: 0.0\n",
      "616 2:03:00 - prob: 0.0\n",
      "617 2:03:20 - prob: 0.0\n",
      "618 2:03:40 - prob: 0.0\n",
      "619 2:03:60 - prob: 0.0\n",
      "620 2:03:80 - prob: 0.0\n",
      "621 2:04:00 - prob: 0.2\n",
      "622 2:04:20 - prob: 0.0\n",
      "623 2:04:40 - prob: 0.0\n",
      "624 2:04:60 - prob: 0.0\n",
      "625 2:04:80 - prob: 0.4\n",
      "626 2:05:00 - prob: 0.0\n",
      "627 2:05:20 - prob: 0.0\n",
      "628 2:05:40 - prob: 0.0\n",
      "629 2:05:60 - prob: 0.0\n",
      "630 2:05:80 - prob: 0.8\n",
      "631 2:06:00 - prob: 28.2\n",
      "632 2:06:20 - prob: 0.0\n",
      "633 2:06:40 - prob: 0.0\n",
      "634 2:06:60 - prob: 0.0\n",
      "635 2:06:80 - prob: 0.0\n",
      "636 2:07:00 - prob: 0.0\n",
      "637 2:07:20 - prob: 0.0\n",
      "638 2:07:40 - prob: 0.0\n",
      "639 2:07:60 - prob: 0.0\n",
      "640 2:07:80 - prob: 7.2\n",
      "641 2:08:00 - prob: 2.3\n",
      "642 2:08:20 - prob: 0.2\n",
      "643 2:08:40 - prob: 0.0\n",
      "644 2:08:60 - prob: 0.0\n",
      "645 2:08:80 - prob: 0.0\n",
      "646 2:09:00 - prob: 0.0\n",
      "647 2:09:20 - prob: 0.0\n",
      "648 2:09:40 - prob: 0.0\n",
      "649 2:09:60 - prob: 0.0\n",
      "650 2:09:80 - prob: 0.0\n",
      "651 2:10:00 - prob: 0.1\n",
      "652 2:10:20 - prob: 0.0\n",
      "653 2:10:40 - prob: 0.0\n",
      "654 2:10:60 - prob: 0.0\n",
      "655 2:10:80 - prob: 0.0\n",
      "656 2:11:00 - prob: 0.0\n",
      "657 2:11:20 - prob: 0.0\n",
      "658 2:11:40 - prob: 0.0\n",
      "659 2:11:60 - prob: 0.0\n",
      "660 2:11:80 - prob: 0.0\n",
      "661 2:12:00 - prob: 0.0\n",
      "662 2:12:20 - prob: 0.0\n",
      "663 2:12:40 - prob: 0.0\n",
      "664 2:12:60 - prob: 0.0\n",
      "665 2:12:80 - prob: 0.0\n",
      "666 2:13:00 - prob: 0.0\n",
      "667 2:13:20 - prob: 0.0\n",
      "668 2:13:40 - prob: 0.0\n",
      "669 2:13:60 - prob: 0.0\n",
      "670 2:13:80 - prob: 0.0\n",
      "671 2:14:00 - prob: 0.0\n",
      "672 2:14:20 - prob: 0.0\n",
      "673 2:14:40 - prob: 0.0\n",
      "674 2:14:60 - prob: 0.0\n",
      "675 2:14:80 - prob: 0.0\n",
      "676 2:15:00 - prob: 0.0\n",
      "677 2:15:20 - prob: 0.0\n",
      "678 2:15:40 - prob: 0.0\n",
      "679 2:15:60 - prob: 0.0\n",
      "680 2:15:80 - prob: 0.0\n",
      "681 2:16:00 - prob: 0.0\n",
      "682 2:16:20 - prob: 0.0\n",
      "683 2:16:40 - prob: 0.5\n",
      "684 2:16:60 - prob: 0.0\n",
      "685 2:16:80 - prob: 0.0\n",
      "686 2:17:00 - prob: 0.0\n",
      "687 2:17:20 - prob: 0.0\n",
      "688 2:17:40 - prob: 0.0\n",
      "689 2:17:60 - prob: 0.0\n",
      "690 2:17:80 - prob: 0.0\n",
      "691 2:18:00 - prob: 0.0\n",
      "692 2:18:20 - prob: 0.0\n",
      "693 2:18:40 - prob: 0.0\n",
      "694 2:18:60 - prob: 0.0\n",
      "695 2:18:80 - prob: 0.0\n",
      "696 2:19:00 - prob: 0.0\n",
      "697 2:19:20 - prob: 0.0\n",
      "698 2:19:40 - prob: 0.0\n",
      "699 2:19:60 - prob: 0.0\n",
      "700 2:19:80 - prob: 0.0\n",
      "701 2:20:00 - prob: 0.0\n",
      "702 2:20:20 - prob: 0.0\n",
      "703 2:20:40 - prob: 0.0\n",
      "704 2:20:60 - prob: 0.0\n",
      "705 2:20:80 - prob: 0.0\n",
      "706 2:21:00 - prob: 0.0\n",
      "707 2:21:20 - prob: 0.0\n",
      "708 2:21:40 - prob: 0.2\n",
      "709 2:21:60 - prob: 0.6\n",
      "710 2:21:80 - prob: 0.0\n",
      "711 2:22:00 - prob: 0.0\n",
      "712 2:22:20 - prob: 0.0\n",
      "713 2:22:40 - prob: 0.0\n",
      "714 2:22:60 - prob: 0.0\n",
      "715 2:22:80 - prob: 0.0\n",
      "716 2:23:00 - prob: 0.0\n",
      "717 2:23:20 - prob: 0.0\n",
      "718 2:23:40 - prob: 0.0\n",
      "719 2:23:60 - prob: 0.0\n",
      "720 2:23:80 - prob: 0.0\n",
      "721 2:24:00 - prob: 0.0\n",
      "722 2:24:20 - prob: 0.0\n",
      "723 2:24:40 - prob: 0.0\n",
      "724 2:24:60 - prob: 0.0\n",
      "725 2:24:80 - prob: 0.0\n",
      "726 2:25:00 - prob: 0.0\n",
      "727 2:25:20 - prob: 0.0\n",
      "728 2:25:40 - prob: 0.0\n",
      "729 2:25:60 - prob: 0.0\n",
      "730 2:25:80 - prob: 0.0\n",
      "731 2:26:00 - prob: 0.0\n",
      "732 2:26:20 - prob: 0.0\n",
      "733 2:26:40 - prob: 0.0\n",
      "734 2:26:60 - prob: 0.0\n",
      "735 2:26:80 - prob: 0.0\n",
      "736 2:27:00 - prob: 0.0\n",
      "737 2:27:20 - prob: 0.0\n",
      "738 2:27:40 - prob: 0.0\n",
      "739 2:27:60 - prob: 0.0\n",
      "740 2:27:80 - prob: 0.0\n",
      "741 2:28:00 - prob: 0.0\n",
      "742 2:28:20 - prob: 0.0\n",
      "743 2:28:40 - prob: 0.0\n",
      "744 2:28:60 - prob: 0.0\n",
      "745 2:28:80 - prob: 0.0\n",
      "746 2:29:00 - prob: 0.2\n",
      "747 2:29:20 - prob: 0.0\n",
      "748 2:29:40 - prob: 0.0\n",
      "749 2:29:60 - prob: 0.0\n",
      "750 2:29:80 - prob: 0.0\n",
      "751 2:30:00 - prob: 0.0\n",
      "752 2:30:20 - prob: 0.0\n",
      "753 2:30:40 - prob: 0.0\n",
      "754 2:30:60 - prob: 0.0\n",
      "755 2:30:80 - prob: 0.0\n",
      "756 2:31:00 - prob: 0.1\n",
      "757 2:31:20 - prob: 0.0\n",
      "758 2:31:40 - prob: 0.0\n",
      "759 2:31:60 - prob: 0.0\n",
      "760 2:31:80 - prob: 0.0\n",
      "761 2:32:00 - prob: 0.0\n",
      "762 2:32:20 - prob: 0.0\n",
      "763 2:32:40 - prob: 0.0\n",
      "764 2:32:60 - prob: 0.0\n",
      "765 2:32:80 - prob: 0.0\n",
      "766 2:33:00 - prob: 0.0\n",
      "767 2:33:20 - prob: 0.0\n",
      "768 2:33:40 - prob: 0.0\n",
      "769 2:33:60 - prob: 0.0\n",
      "770 2:33:80 - prob: 0.0\n",
      "771 2:34:00 - prob: 0.0\n",
      "772 2:34:20 - prob: 0.0\n",
      "773 2:34:40 - prob: 0.0\n",
      "774 2:34:60 - prob: 0.0\n",
      "775 2:34:80 - prob: 0.0\n",
      "776 2:35:00 - prob: 0.0\n",
      "777 2:35:20 - prob: 0.0\n",
      "778 2:35:40 - prob: 0.0\n",
      "779 2:35:60 - prob: 0.1\n",
      "780 2:35:80 - prob: 0.0\n",
      "781 2:36:00 - prob: 0.0\n",
      "782 2:36:20 - prob: 0.0\n",
      "783 2:36:40 - prob: 0.0\n",
      "784 2:36:60 - prob: 0.0\n",
      "785 2:36:80 - prob: 0.0\n",
      "786 2:37:00 - prob: 0.0\n",
      "787 2:37:20 - prob: 0.0\n",
      "788 2:37:40 - prob: 0.0\n",
      "789 2:37:60 - prob: 0.0\n",
      "790 2:37:80 - prob: 0.0\n",
      "791 2:38:00 - prob: 0.2\n",
      "792 2:38:20 - prob: 0.0\n",
      "793 2:38:40 - prob: 0.0\n",
      "794 2:38:60 - prob: 0.0\n",
      "795 2:38:80 - prob: 0.0\n",
      "796 2:39:00 - prob: 0.0\n",
      "797 2:39:20 - prob: 0.0\n",
      "798 2:39:40 - prob: 0.0\n",
      "799 2:39:60 - prob: 0.1\n",
      "800 2:39:80 - prob: 0.0\n",
      "801 2:40:00 - prob: 0.0\n",
      "802 2:40:20 - prob: 0.0\n",
      "803 2:40:40 - prob: 0.0\n",
      "804 2:40:60 - prob: 0.0\n",
      "805 2:40:80 - prob: 0.2\n",
      "806 2:41:00 - prob: 0.1\n",
      "807 2:41:20 - prob: 0.0\n",
      "808 2:41:40 - prob: 0.0\n",
      "809 2:41:60 - prob: 0.0\n",
      "810 2:41:80 - prob: 0.0\n",
      "811 2:42:00 - prob: 0.0\n",
      "812 2:42:20 - prob: 0.0\n",
      "813 2:42:40 - prob: 0.0\n",
      "814 2:42:60 - prob: 0.0\n",
      "815 2:42:80 - prob: 0.0\n",
      "816 2:43:00 - prob: 0.0\n",
      "817 2:43:20 - prob: 0.0\n",
      "818 2:43:40 - prob: 0.0\n",
      "819 2:43:60 - prob: 0.0\n",
      "820 2:43:80 - prob: 0.0\n",
      "821 2:44:00 - prob: 0.0\n",
      "822 2:44:20 - prob: 0.0\n",
      "823 2:44:40 - prob: 2.1\n",
      "824 2:44:60 - prob: 5.8\n",
      "825 2:44:80 - prob: 0.0\n",
      "826 2:45:00 - prob: 0.0\n",
      "827 2:45:20 - prob: 0.0\n",
      "828 2:45:40 - prob: 0.1\n",
      "829 2:45:60 - prob: 0.0\n",
      "830 2:45:80 - prob: 22.1\n",
      "831 2:46:00 - prob: 5.1\n",
      "832 2:46:20 - prob: 0.0\n",
      "833 2:46:40 - prob: 0.0\n",
      "834 2:46:60 - prob: 6.1\n",
      "835 2:46:80 - prob: 0.0\n",
      "836 2:47:00 - prob: 0.0\n",
      "837 2:47:20 - prob: 0.0\n",
      "838 2:47:40 - prob: 0.9\n",
      "839 2:47:60 - prob: 0.0\n",
      "840 2:47:80 - prob: 0.0\n",
      "841 2:48:00 - prob: 0.0\n",
      "842 2:48:20 - prob: 0.0\n",
      "843 2:48:40 - prob: 0.0\n",
      "844 2:48:60 - prob: 0.0\n",
      "845 2:48:80 - prob: 0.0\n",
      "846 2:49:00 - prob: 1.7\n",
      "847 2:49:20 - prob: 0.2\n",
      "848 2:49:40 - prob: 0.0\n",
      "849 2:49:60 - prob: 0.0\n",
      "850 2:49:80 - prob: 11.1\n",
      "851 2:50:00 - prob: 0.1\n",
      "852 2:50:20 - prob: 0.1\n",
      "853 2:50:40 - prob: 0.0\n",
      "854 2:50:60 - prob: 0.0\n",
      "855 2:50:80 - prob: 0.0\n",
      "856 2:51:00 - prob: 0.0\n",
      "857 2:51:20 - prob: 0.0\n",
      "858 2:51:40 - prob: 0.0\n",
      "859 2:51:60 - prob: 0.0\n",
      "860 2:51:80 - prob: 0.0\n",
      "861 2:52:00 - prob: 0.0\n",
      "862 2:52:20 - prob: 0.0\n",
      "863 2:52:40 - prob: 0.0\n",
      "864 2:52:60 - prob: 0.0\n",
      "865 2:52:80 - prob: 0.0\n",
      "866 2:53:00 - prob: 0.1\n",
      "867 2:53:20 - prob: 0.0\n",
      "868 2:53:40 - prob: 0.0\n",
      "869 2:53:60 - prob: 0.0\n",
      "870 2:53:80 - prob: 0.0\n",
      "871 2:54:00 - prob: 0.0\n",
      "872 2:54:20 - prob: 0.0\n",
      "873 2:54:40 - prob: 0.0\n",
      "874 2:54:60 - prob: 0.0\n",
      "875 2:54:80 - prob: 0.0\n",
      "876 2:55:00 - prob: 0.0\n",
      "877 2:55:20 - prob: 0.0\n",
      "878 2:55:40 - prob: 0.0\n",
      "879 2:55:60 - prob: 0.0\n",
      "880 2:55:80 - prob: 0.0\n",
      "881 2:56:00 - prob: 0.0\n",
      "882 2:56:20 - prob: 0.0\n",
      "883 2:56:40 - prob: 0.0\n",
      "884 2:56:60 - prob: 0.0\n",
      "885 2:56:80 - prob: 0.0\n",
      "886 2:57:00 - prob: 0.0\n",
      "887 2:57:20 - prob: 0.0\n",
      "888 2:57:40 - prob: 0.0\n",
      "889 2:57:60 - prob: 0.0\n",
      "890 2:57:80 - prob: 0.0\n",
      "891 2:58:00 - prob: 0.0\n",
      "892 2:58:20 - prob: 0.0\n",
      "893 2:58:40 - prob: 0.0\n",
      "894 2:58:60 - prob: 0.0\n",
      "895 2:58:80 - prob: 0.2\n",
      "896 2:59:00 - prob: 0.0\n",
      "897 2:59:20 - prob: 0.0\n",
      "898 2:59:40 - prob: 0.0\n",
      "899 2:59:60 - prob: 0.0\n",
      "900 2:59:80 - prob: 0.0\n",
      "901 3:00:00 - prob: 0.0\n",
      "902 3:00:20 - prob: 0.0\n",
      "903 3:00:40 - prob: 0.0\n",
      "904 3:00:60 - prob: 0.0\n",
      "905 3:00:80 - prob: 0.0\n",
      "906 3:01:00 - prob: 0.0\n",
      "907 3:01:20 - prob: 0.0\n",
      "908 3:01:40 - prob: 0.0\n",
      "909 3:01:60 - prob: 0.0\n",
      "910 3:01:80 - prob: 0.0\n",
      "911 3:02:00 - prob: 0.0\n",
      "912 3:02:20 - prob: 0.1\n",
      "913 3:02:40 - prob: 0.2\n",
      "914 3:02:60 - prob: 0.2\n",
      "915 3:02:80 - prob: 70.0\n",
      "916 3:03:00 - prob: 99.9\n",
      "917 3:03:20 - prob: 100.0\n",
      "918 3:03:40 - prob: 100.0\n",
      "919 3:03:60 - prob: 99.9\n",
      "920 3:03:80 - prob: 0.2\n",
      "921 3:04:00 - prob: 0.1\n",
      "922 3:04:20 - prob: 0.0\n",
      "923 3:04:40 - prob: 0.0\n",
      "924 3:04:60 - prob: 0.0\n",
      "925 3:04:80 - prob: 0.0\n",
      "926 3:05:00 - prob: 0.0\n",
      "927 3:05:20 - prob: 0.0\n",
      "928 3:05:40 - prob: 0.0\n",
      "929 3:05:60 - prob: 0.0\n",
      "930 3:05:80 - prob: 0.0\n",
      "931 3:06:00 - prob: 0.0\n",
      "932 3:06:20 - prob: 0.0\n",
      "933 3:06:40 - prob: 0.0\n",
      "934 3:06:60 - prob: 0.0\n",
      "935 3:06:80 - prob: 0.4\n",
      "936 3:07:00 - prob: 0.0\n",
      "937 3:07:20 - prob: 0.0\n",
      "938 3:07:40 - prob: 0.0\n",
      "939 3:07:60 - prob: 0.0\n",
      "940 3:07:80 - prob: 0.0\n",
      "941 3:08:00 - prob: 0.0\n",
      "942 3:08:20 - prob: 0.0\n",
      "943 3:08:40 - prob: 0.0\n",
      "944 3:08:60 - prob: 0.0\n",
      "945 3:08:80 - prob: 0.0\n",
      "946 3:09:00 - prob: 0.0\n",
      "947 3:09:20 - prob: 0.0\n",
      "948 3:09:40 - prob: 0.1\n",
      "949 3:09:60 - prob: 0.0\n",
      "950 3:09:80 - prob: 0.0\n",
      "951 3:10:00 - prob: 0.0\n",
      "952 3:10:20 - prob: 0.0\n",
      "953 3:10:40 - prob: 0.0\n",
      "954 3:10:60 - prob: 0.0\n",
      "955 3:10:80 - prob: 0.0\n",
      "956 3:11:00 - prob: 0.0\n",
      "957 3:11:20 - prob: 0.0\n",
      "958 3:11:40 - prob: 0.0\n",
      "959 3:11:60 - prob: 0.0\n",
      "960 3:11:80 - prob: 0.0\n",
      "961 3:12:00 - prob: 0.0\n",
      "962 3:12:20 - prob: 0.1\n",
      "963 3:12:40 - prob: 0.0\n",
      "964 3:12:60 - prob: 0.0\n",
      "965 3:12:80 - prob: 0.0\n",
      "966 3:13:00 - prob: 0.0\n",
      "967 3:13:20 - prob: 0.0\n",
      "968 3:13:40 - prob: 0.0\n",
      "969 3:13:60 - prob: 0.0\n",
      "970 3:13:80 - prob: 0.0\n",
      "971 3:14:00 - prob: 0.4\n",
      "972 3:14:20 - prob: 2.2\n",
      "973 3:14:40 - prob: 0.3\n",
      "974 3:14:60 - prob: 31.7\n",
      "975 3:14:80 - prob: 0.5\n",
      "976 3:15:00 - prob: 0.0\n",
      "977 3:15:20 - prob: 0.0\n",
      "978 3:15:40 - prob: 0.0\n",
      "979 3:15:60 - prob: 0.0\n",
      "980 3:15:80 - prob: 0.0\n",
      "981 3:16:00 - prob: 0.0\n",
      "982 3:16:20 - prob: 0.0\n",
      "983 3:16:40 - prob: 0.0\n",
      "984 3:16:60 - prob: 0.0\n",
      "985 3:16:80 - prob: 0.0\n",
      "986 3:17:00 - prob: 2.9\n",
      "987 3:17:20 - prob: 0.0\n",
      "988 3:17:40 - prob: 0.0\n",
      "989 3:17:60 - prob: 0.0\n",
      "990 3:17:80 - prob: 0.0\n",
      "991 3:18:00 - prob: 0.0\n",
      "992 3:18:20 - prob: 0.0\n",
      "993 3:18:40 - prob: 0.0\n",
      "994 3:18:60 - prob: 0.0\n",
      "995 3:18:80 - prob: 0.0\n",
      "996 3:19:00 - prob: 0.0\n",
      "997 3:19:20 - prob: 0.0\n",
      "998 3:19:40 - prob: 0.6\n",
      "999 3:19:60 - prob: 0.0\n",
      "1000 3:19:80 - prob: 0.1\n",
      "1001 3:20:00 - prob: 0.0\n",
      "1002 3:20:20 - prob: 0.2\n",
      "1003 3:20:40 - prob: 0.1\n",
      "1004 3:20:60 - prob: 1.0\n",
      "1005 3:20:80 - prob: 0.0\n",
      "1006 3:21:00 - prob: 0.0\n",
      "1007 3:21:20 - prob: 0.0\n",
      "1008 3:21:40 - prob: 0.0\n",
      "1009 3:21:60 - prob: 0.0\n",
      "1010 3:21:80 - prob: 0.0\n",
      "1011 3:22:00 - prob: 0.0\n",
      "1012 3:22:20 - prob: 0.0\n",
      "1013 3:22:40 - prob: 0.0\n",
      "1014 3:22:60 - prob: 0.0\n",
      "1015 3:22:80 - prob: 17.8\n",
      "1016 3:23:00 - prob: 15.3\n",
      "1017 3:23:20 - prob: 0.0\n",
      "1018 3:23:40 - prob: 0.0\n",
      "1019 3:23:60 - prob: 0.0\n",
      "1020 3:23:80 - prob: 0.0\n",
      "1021 3:24:00 - prob: 0.0\n",
      "1022 3:24:20 - prob: 0.0\n",
      "1023 3:24:40 - prob: 0.0\n",
      "1024 3:24:60 - prob: 0.0\n",
      "1025 3:24:80 - prob: 0.0\n",
      "1026 3:25:00 - prob: 0.0\n",
      "1027 3:25:20 - prob: 0.0\n",
      "1028 3:25:40 - prob: 0.0\n",
      "1029 3:25:60 - prob: 0.0\n",
      "1030 3:25:80 - prob: 0.0\n",
      "1031 3:26:00 - prob: 0.0\n",
      "1032 3:26:20 - prob: 0.0\n",
      "1033 3:26:40 - prob: 0.0\n",
      "1034 3:26:60 - prob: 0.0\n",
      "1035 3:26:80 - prob: 0.0\n",
      "1036 3:27:00 - prob: 0.0\n",
      "1037 3:27:20 - prob: 0.0\n",
      "1038 3:27:40 - prob: 0.0\n",
      "1039 3:27:60 - prob: 0.0\n",
      "1040 3:27:80 - prob: 0.0\n",
      "1041 3:28:00 - prob: 0.0\n",
      "1042 3:28:20 - prob: 2.8\n",
      "1043 3:28:40 - prob: 0.0\n",
      "1044 3:28:60 - prob: 0.0\n",
      "1045 3:28:80 - prob: 0.0\n",
      "1046 3:29:00 - prob: 0.0\n",
      "1047 3:29:20 - prob: 0.0\n",
      "1048 3:29:40 - prob: 0.0\n",
      "1049 3:29:60 - prob: 0.0\n",
      "1050 3:29:80 - prob: 0.0\n",
      "1051 3:30:00 - prob: 0.0\n",
      "1052 3:30:20 - prob: 0.0\n",
      "1053 3:30:40 - prob: 0.0\n",
      "1054 3:30:60 - prob: 0.0\n",
      "1055 3:30:80 - prob: 0.0\n",
      "1056 3:31:00 - prob: 0.0\n",
      "1057 3:31:20 - prob: 0.0\n",
      "1058 3:31:40 - prob: 0.0\n",
      "1059 3:31:60 - prob: 0.0\n",
      "1060 3:31:80 - prob: 0.7\n",
      "1061 3:32:00 - prob: 1.6\n",
      "1062 3:32:20 - prob: 0.0\n",
      "1063 3:32:40 - prob: 0.0\n",
      "1064 3:32:60 - prob: 0.0\n",
      "1065 3:32:80 - prob: 0.1\n",
      "1066 3:33:00 - prob: 0.0\n",
      "1067 3:33:20 - prob: 0.0\n",
      "1068 3:33:40 - prob: 0.0\n",
      "1069 3:33:60 - prob: 0.0\n",
      "1070 3:33:80 - prob: 0.0\n",
      "1071 3:34:00 - prob: 0.0\n",
      "1072 3:34:20 - prob: 0.0\n",
      "1073 3:34:40 - prob: 0.0\n",
      "1074 3:34:60 - prob: 0.0\n",
      "1075 3:34:80 - prob: 0.0\n",
      "1076 3:35:00 - prob: 0.1\n",
      "1077 3:35:20 - prob: 0.0\n",
      "1078 3:35:40 - prob: 0.1\n",
      "1079 3:35:60 - prob: 0.3\n",
      "1080 3:35:80 - prob: 0.0\n",
      "1081 3:36:00 - prob: 0.0\n",
      "1082 3:36:20 - prob: 0.0\n",
      "1083 3:36:40 - prob: 0.0\n",
      "1084 3:36:60 - prob: 0.0\n",
      "1085 3:36:80 - prob: 0.0\n",
      "1086 3:37:00 - prob: 0.0\n",
      "1087 3:37:20 - prob: 0.0\n",
      "1088 3:37:40 - prob: 0.0\n",
      "1089 3:37:60 - prob: 0.0\n",
      "1090 3:37:80 - prob: 0.0\n",
      "1091 3:38:00 - prob: 3.8\n",
      "1092 3:38:20 - prob: 57.2\n",
      "1093 3:38:40 - prob: 74.8\n",
      "1094 3:38:60 - prob: 3.2\n",
      "1095 3:38:80 - prob: 83.2\n",
      "1096 3:39:00 - prob: 99.6\n",
      "1097 3:39:20 - prob: 1.0\n",
      "1098 3:39:40 - prob: 0.0\n",
      "1099 3:39:60 - prob: 0.0\n",
      "1100 3:39:80 - prob: 1.6\n",
      "1101 3:40:00 - prob: 100.0\n",
      "1102 3:40:20 - prob: 100.0\n",
      "1103 3:40:40 - prob: 99.9\n",
      "1104 3:40:60 - prob: 0.0\n",
      "1105 3:40:80 - prob: 0.0\n",
      "1106 3:41:00 - prob: 0.0\n",
      "1107 3:41:20 - prob: 0.0\n",
      "1108 3:41:40 - prob: 0.0\n",
      "1109 3:41:60 - prob: 0.0\n",
      "1110 3:41:80 - prob: 1.0\n",
      "1111 3:42:00 - prob: 2.7\n",
      "1112 3:42:20 - prob: 0.0\n",
      "1113 3:42:40 - prob: 0.0\n",
      "1114 3:42:60 - prob: 0.0\n",
      "1115 3:42:80 - prob: 0.0\n",
      "1116 3:43:00 - prob: 0.0\n",
      "1117 3:43:20 - prob: 0.0\n",
      "1118 3:43:40 - prob: 0.0\n",
      "1119 3:43:60 - prob: 0.0\n",
      "1120 3:43:80 - prob: 0.0\n",
      "1121 3:44:00 - prob: 99.9\n",
      "1122 3:44:20 - prob: 100.0\n",
      "1123 3:44:40 - prob: 100.0\n",
      "1124 3:44:60 - prob: 4.1\n",
      "1125 3:44:80 - prob: 0.0\n",
      "1126 3:45:00 - prob: 0.0\n",
      "1127 3:45:20 - prob: 0.0\n",
      "1128 3:45:40 - prob: 1.1\n",
      "1129 3:45:60 - prob: 0.5\n",
      "1130 3:45:80 - prob: 73.9\n",
      "1131 3:46:00 - prob: 55.9\n",
      "1132 3:46:20 - prob: 10.8\n",
      "1133 3:46:40 - prob: 0.0\n",
      "1134 3:46:60 - prob: 0.0\n",
      "1135 3:46:80 - prob: 0.0\n",
      "1136 3:47:00 - prob: 0.0\n",
      "1137 3:47:20 - prob: 0.0\n",
      "1138 3:47:40 - prob: 0.0\n",
      "1139 3:47:60 - prob: 0.0\n",
      "1140 3:47:80 - prob: 0.0\n",
      "1141 3:48:00 - prob: 0.0\n",
      "1142 3:48:20 - prob: 0.0\n",
      "1143 3:48:40 - prob: 0.0\n",
      "1144 3:48:60 - prob: 2.2\n",
      "1145 3:48:80 - prob: 0.0\n",
      "1146 3:49:00 - prob: 0.1\n",
      "1147 3:49:20 - prob: 0.0\n",
      "1148 3:49:40 - prob: 0.0\n",
      "1149 3:49:60 - prob: 0.0\n",
      "1150 3:49:80 - prob: 0.0\n",
      "1151 3:50:00 - prob: 0.0\n",
      "1152 3:50:20 - prob: 0.0\n",
      "1153 3:50:40 - prob: 0.0\n",
      "1154 3:50:60 - prob: 0.0\n",
      "1155 3:50:80 - prob: 0.0\n",
      "1156 3:51:00 - prob: 0.0\n",
      "1157 3:51:20 - prob: 0.0\n",
      "1158 3:51:40 - prob: 0.0\n",
      "1159 3:51:60 - prob: 0.0\n",
      "1160 3:51:80 - prob: 0.0\n",
      "1161 3:52:00 - prob: 0.0\n",
      "1162 3:52:20 - prob: 0.0\n",
      "1163 3:52:40 - prob: 0.0\n",
      "1164 3:52:60 - prob: 0.0\n",
      "1165 3:52:80 - prob: 0.0\n",
      "1166 3:53:00 - prob: 0.0\n",
      "1167 3:53:20 - prob: 0.0\n",
      "1168 3:53:40 - prob: 0.0\n",
      "1169 3:53:60 - prob: 0.0\n",
      "1170 3:53:80 - prob: 0.0\n",
      "1171 3:54:00 - prob: 0.0\n",
      "1172 3:54:20 - prob: 0.0\n",
      "1173 3:54:40 - prob: 0.0\n",
      "1174 3:54:60 - prob: 0.0\n",
      "1175 3:54:80 - prob: 0.0\n",
      "1176 3:55:00 - prob: 0.1\n",
      "1177 3:55:20 - prob: 6.6\n",
      "1178 3:55:40 - prob: 0.4\n",
      "1179 3:55:60 - prob: 0.0\n",
      "1180 3:55:80 - prob: 0.0\n",
      "1181 3:56:00 - prob: 0.0\n",
      "1182 3:56:20 - prob: 0.0\n",
      "1183 3:56:40 - prob: 0.0\n",
      "1184 3:56:60 - prob: 0.0\n",
      "1185 3:56:80 - prob: 0.0\n",
      "1186 3:57:00 - prob: 0.0\n",
      "1187 3:57:20 - prob: 0.0\n",
      "1188 3:57:40 - prob: 0.0\n",
      "1189 3:57:60 - prob: 0.0\n",
      "1190 3:57:80 - prob: 0.0\n",
      "1191 3:58:00 - prob: 0.0\n",
      "1192 3:58:20 - prob: 0.0\n",
      "1193 3:58:40 - prob: 0.0\n",
      "1194 3:58:60 - prob: 0.0\n",
      "1195 3:58:80 - prob: 0.0\n",
      "1196 3:59:00 - prob: 0.0\n",
      "1197 3:59:20 - prob: 0.0\n",
      "1198 3:59:40 - prob: 0.0\n",
      "1199 3:59:60 - prob: 0.0\n",
      "1200 3:59:80 - prob: 0.1\n",
      "1201 4:00:00 - prob: 0.0\n",
      "1202 4:00:20 - prob: 0.0\n",
      "1203 4:00:40 - prob: 0.0\n",
      "1204 4:00:60 - prob: 0.0\n",
      "1205 4:00:80 - prob: 0.0\n",
      "1206 4:01:00 - prob: 0.0\n",
      "1207 4:01:20 - prob: 0.0\n",
      "1208 4:01:40 - prob: 0.0\n",
      "1209 4:01:60 - prob: 0.0\n",
      "1210 4:01:80 - prob: 0.0\n",
      "1211 4:02:00 - prob: 0.0\n",
      "1212 4:02:20 - prob: 0.0\n",
      "1213 4:02:40 - prob: 0.0\n",
      "1214 4:02:60 - prob: 3.6\n",
      "1215 4:02:80 - prob: 0.0\n",
      "1216 4:03:00 - prob: 0.5\n",
      "1217 4:03:20 - prob: 0.0\n",
      "1218 4:03:40 - prob: 0.0\n",
      "1219 4:03:60 - prob: 0.0\n",
      "1220 4:03:80 - prob: 0.0\n",
      "1221 4:04:00 - prob: 0.0\n",
      "1222 4:04:20 - prob: 0.0\n",
      "1223 4:04:40 - prob: 0.0\n",
      "1224 4:04:60 - prob: 0.0\n",
      "1225 4:04:80 - prob: 0.0\n",
      "1226 4:05:00 - prob: 0.0\n",
      "1227 4:05:20 - prob: 0.0\n",
      "1228 4:05:40 - prob: 0.0\n",
      "1229 4:05:60 - prob: 0.0\n",
      "1230 4:05:80 - prob: 0.0\n",
      "1231 4:06:00 - prob: 0.0\n",
      "1232 4:06:20 - prob: 0.0\n",
      "1233 4:06:40 - prob: 0.0\n",
      "1234 4:06:60 - prob: 0.0\n",
      "1235 4:06:80 - prob: 0.0\n",
      "1236 4:07:00 - prob: 0.0\n",
      "1237 4:07:20 - prob: 0.0\n",
      "1238 4:07:40 - prob: 0.0\n",
      "1239 4:07:60 - prob: 0.0\n",
      "1240 4:07:80 - prob: 99.6\n",
      "1241 4:08:00 - prob: 100.0\n",
      "1242 4:08:20 - prob: 100.0\n",
      "1243 4:08:40 - prob: 100.0\n",
      "1244 4:08:60 - prob: 5.1\n",
      "1245 4:08:80 - prob: 21.5\n",
      "1246 4:09:00 - prob: 58.9\n",
      "1247 4:09:20 - prob: 85.9\n",
      "1248 4:09:40 - prob: 3.9\n",
      "1249 4:09:60 - prob: 0.2\n",
      "1250 4:09:80 - prob: 0.0\n",
      "1251 4:10:00 - prob: 0.0\n",
      "1252 4:10:20 - prob: 0.0\n",
      "1253 4:10:40 - prob: 0.0\n",
      "1254 4:10:60 - prob: 0.0\n",
      "1255 4:10:80 - prob: 0.2\n",
      "1256 4:11:00 - prob: 2.8\n",
      "1257 4:11:20 - prob: 0.0\n",
      "1258 4:11:40 - prob: 0.0\n",
      "1259 4:11:60 - prob: 0.0\n",
      "1260 4:11:80 - prob: 0.0\n",
      "1261 4:12:00 - prob: 0.0\n",
      "1262 4:12:20 - prob: 0.0\n",
      "1263 4:12:40 - prob: 0.0\n",
      "1264 4:12:60 - prob: 0.0\n",
      "1265 4:12:80 - prob: 0.0\n",
      "1266 4:13:00 - prob: 0.0\n",
      "1267 4:13:20 - prob: 0.0\n",
      "1268 4:13:40 - prob: 0.0\n",
      "1269 4:13:60 - prob: 0.0\n",
      "1270 4:13:80 - prob: 0.0\n",
      "1271 4:14:00 - prob: 0.0\n",
      "1272 4:14:20 - prob: 0.0\n",
      "1273 4:14:40 - prob: 0.0\n",
      "1274 4:14:60 - prob: 0.0\n",
      "1275 4:14:80 - prob: 0.0\n",
      "1276 4:15:00 - prob: 0.1\n",
      "1277 4:15:20 - prob: 0.0\n",
      "1278 4:15:40 - prob: 0.0\n",
      "1279 4:15:60 - prob: 0.0\n",
      "1280 4:15:80 - prob: 0.0\n",
      "1281 4:16:00 - prob: 1.4\n",
      "1282 4:16:20 - prob: 2.0\n",
      "1283 4:16:40 - prob: 0.0\n",
      "1284 4:16:60 - prob: 0.0\n",
      "1285 4:16:80 - prob: 0.0\n",
      "1286 4:17:00 - prob: 0.0\n",
      "1287 4:17:20 - prob: 0.0\n",
      "1288 4:17:40 - prob: 0.0\n",
      "1289 4:17:60 - prob: 0.0\n",
      "1290 4:17:80 - prob: 0.0\n",
      "1291 4:18:00 - prob: 0.0\n",
      "1292 4:18:20 - prob: 0.1\n",
      "1293 4:18:40 - prob: 1.5\n",
      "1294 4:18:60 - prob: 67.0\n",
      "1295 4:18:80 - prob: 0.5\n",
      "1296 4:19:00 - prob: 0.8\n",
      "1297 4:19:20 - prob: 0.0\n",
      "1298 4:19:40 - prob: 15.1\n",
      "1299 4:19:60 - prob: 0.0\n",
      "1300 4:19:80 - prob: 0.0\n",
      "1301 4:20:00 - prob: 0.0\n",
      "1302 4:20:20 - prob: 0.0\n",
      "1303 4:20:40 - prob: 0.0\n",
      "1304 4:20:60 - prob: 0.0\n",
      "1305 4:20:80 - prob: 0.0\n",
      "1306 4:21:00 - prob: 0.0\n",
      "1307 4:21:20 - prob: 0.0\n",
      "1308 4:21:40 - prob: 0.0\n",
      "1309 4:21:60 - prob: 0.0\n",
      "1310 4:21:80 - prob: 0.0\n",
      "1311 4:22:00 - prob: 0.0\n",
      "1312 4:22:20 - prob: 0.0\n",
      "1313 4:22:40 - prob: 0.0\n",
      "1314 4:22:60 - prob: 0.0\n",
      "1315 4:22:80 - prob: 0.0\n",
      "1316 4:23:00 - prob: 0.0\n",
      "1317 4:23:20 - prob: 0.0\n",
      "1318 4:23:40 - prob: 0.0\n",
      "1319 4:23:60 - prob: 0.0\n",
      "1320 4:23:80 - prob: 0.0\n",
      "1321 4:24:00 - prob: 0.0\n",
      "1322 4:24:20 - prob: 0.0\n",
      "1323 4:24:40 - prob: 0.9\n",
      "1324 4:24:60 - prob: 0.1\n",
      "1325 4:24:80 - prob: 0.0\n",
      "1326 4:25:00 - prob: 0.0\n",
      "1327 4:25:20 - prob: 0.0\n",
      "1328 4:25:40 - prob: 2.9\n",
      "1329 4:25:60 - prob: 0.0\n",
      "1330 4:25:80 - prob: 0.0\n",
      "1331 4:26:00 - prob: 0.0\n",
      "1332 4:26:20 - prob: 0.0\n",
      "1333 4:26:40 - prob: 0.0\n",
      "1334 4:26:60 - prob: 0.0\n",
      "1335 4:26:80 - prob: 0.0\n",
      "1336 4:27:00 - prob: 0.0\n",
      "1337 4:27:20 - prob: 0.0\n",
      "1338 4:27:40 - prob: 0.0\n",
      "1339 4:27:60 - prob: 0.2\n",
      "1340 4:27:80 - prob: 0.5\n",
      "1341 4:28:00 - prob: 0.0\n",
      "1342 4:28:20 - prob: 0.0\n",
      "1343 4:28:40 - prob: 0.0\n",
      "1344 4:28:60 - prob: 0.0\n",
      "1345 4:28:80 - prob: 0.2\n",
      "1346 4:29:00 - prob: 0.5\n",
      "1347 4:29:20 - prob: 0.0\n",
      "1348 4:29:40 - prob: 0.0\n",
      "1349 4:29:60 - prob: 0.0\n",
      "1350 4:29:80 - prob: 0.0\n",
      "1351 4:30:00 - prob: 0.0\n",
      "1352 4:30:20 - prob: 0.0\n",
      "1353 4:30:40 - prob: 0.0\n",
      "1354 4:30:60 - prob: 15.8\n",
      "1355 4:30:80 - prob: 0.6\n",
      "1356 4:31:00 - prob: 39.7\n",
      "1357 4:31:20 - prob: 0.0\n",
      "1358 4:31:40 - prob: 0.0\n",
      "1359 4:31:60 - prob: 0.0\n",
      "1360 4:31:80 - prob: 0.3\n",
      "1361 4:32:00 - prob: 0.0\n",
      "1362 4:32:20 - prob: 0.2\n",
      "1363 4:32:40 - prob: 0.0\n",
      "1364 4:32:60 - prob: 0.0\n",
      "1365 4:32:80 - prob: 0.0\n",
      "1366 4:33:00 - prob: 99.7\n",
      "1367 4:33:20 - prob: 99.9\n",
      "1368 4:33:40 - prob: 72.4\n",
      "1369 4:33:60 - prob: 0.0\n",
      "1370 4:33:80 - prob: 0.0\n",
      "1371 4:34:00 - prob: 0.1\n",
      "1372 4:34:20 - prob: 91.3\n",
      "1373 4:34:40 - prob: 98.1\n",
      "1374 4:34:60 - prob: 1.9\n",
      "1375 4:34:80 - prob: 0.0\n",
      "1376 4:35:00 - prob: 0.0\n",
      "1377 4:35:20 - prob: 0.0\n",
      "1378 4:35:40 - prob: 0.1\n",
      "1379 4:35:60 - prob: 0.0\n",
      "1380 4:35:80 - prob: 0.0\n",
      "1381 4:36:00 - prob: 0.0\n",
      "1382 4:36:20 - prob: 0.2\n",
      "1383 4:36:40 - prob: 3.8\n",
      "1384 4:36:60 - prob: 61.6\n",
      "1385 4:36:80 - prob: 0.0\n",
      "1386 4:37:00 - prob: 0.0\n",
      "1387 4:37:20 - prob: 0.0\n",
      "1388 4:37:40 - prob: 0.0\n",
      "1389 4:37:60 - prob: 0.0\n",
      "1390 4:37:80 - prob: 0.5\n",
      "1391 4:38:00 - prob: 0.1\n",
      "1392 4:38:20 - prob: 0.0\n",
      "1393 4:38:40 - prob: 0.0\n",
      "1394 4:38:60 - prob: 0.0\n",
      "1395 4:38:80 - prob: 0.0\n",
      "1396 4:39:00 - prob: 0.0\n",
      "1397 4:39:20 - prob: 0.0\n",
      "1398 4:39:40 - prob: 0.0\n",
      "1399 4:39:60 - prob: 0.0\n",
      "1400 4:39:80 - prob: 0.0\n",
      "1401 4:40:00 - prob: 0.0\n",
      "1402 4:40:20 - prob: 0.0\n",
      "1403 4:40:40 - prob: 0.0\n",
      "1404 4:40:60 - prob: 0.0\n",
      "1405 4:40:80 - prob: 0.4\n",
      "1406 4:41:00 - prob: 0.0\n",
      "1407 4:41:20 - prob: 0.0\n",
      "1408 4:41:40 - prob: 0.0\n",
      "1409 4:41:60 - prob: 0.0\n",
      "1410 4:41:80 - prob: 0.0\n",
      "1411 4:42:00 - prob: 0.0\n",
      "1412 4:42:20 - prob: 0.0\n",
      "1413 4:42:40 - prob: 0.0\n",
      "1414 4:42:60 - prob: 0.0\n",
      "1415 4:42:80 - prob: 0.0\n",
      "1416 4:43:00 - prob: 0.0\n",
      "1417 4:43:20 - prob: 0.0\n",
      "1418 4:43:40 - prob: 0.0\n",
      "1419 4:43:60 - prob: 0.0\n",
      "1420 4:43:80 - prob: 0.1\n",
      "1421 4:44:00 - prob: 0.0\n",
      "1422 4:44:20 - prob: 0.0\n",
      "1423 4:44:40 - prob: 0.0\n",
      "1424 4:44:60 - prob: 0.0\n",
      "1425 4:44:80 - prob: 0.0\n",
      "1426 4:45:00 - prob: 0.0\n",
      "1427 4:45:20 - prob: 0.0\n",
      "1428 4:45:40 - prob: 0.0\n",
      "1429 4:45:60 - prob: 0.0\n",
      "1430 4:45:80 - prob: 0.0\n",
      "1431 4:46:00 - prob: 0.0\n",
      "1432 4:46:20 - prob: 1.0\n",
      "1433 4:46:40 - prob: 0.0\n",
      "1434 4:46:60 - prob: 0.0\n",
      "1435 4:46:80 - prob: 0.0\n",
      "1436 4:47:00 - prob: 0.0\n",
      "1437 4:47:20 - prob: 0.0\n",
      "1438 4:47:40 - prob: 0.1\n",
      "1439 4:47:60 - prob: 0.0\n",
      "1440 4:47:80 - prob: 0.0\n",
      "1441 4:48:00 - prob: 0.0\n",
      "1442 4:48:20 - prob: 0.0\n",
      "1443 4:48:40 - prob: 0.0\n",
      "1444 4:48:60 - prob: 0.0\n",
      "1445 4:48:80 - prob: 0.0\n",
      "1446 4:49:00 - prob: 0.0\n",
      "1447 4:49:20 - prob: 0.0\n",
      "1448 4:49:40 - prob: 0.0\n",
      "1449 4:49:60 - prob: 0.0\n",
      "1450 4:49:80 - prob: 0.0\n",
      "1451 4:50:00 - prob: 0.0\n",
      "1452 4:50:20 - prob: 0.0\n",
      "1453 4:50:40 - prob: 0.0\n",
      "1454 4:50:60 - prob: 0.0\n",
      "1455 4:50:80 - prob: 0.0\n",
      "1456 4:51:00 - prob: 0.0\n",
      "1457 4:51:20 - prob: 0.0\n",
      "1458 4:51:40 - prob: 0.0\n",
      "1459 4:51:60 - prob: 0.0\n",
      "1460 4:51:80 - prob: 0.0\n",
      "1461 4:52:00 - prob: 0.0\n",
      "1462 4:52:20 - prob: 0.0\n",
      "1463 4:52:40 - prob: 0.0\n",
      "1464 4:52:60 - prob: 0.6\n",
      "1465 4:52:80 - prob: 0.0\n",
      "1466 4:53:00 - prob: 0.0\n",
      "1467 4:53:20 - prob: 0.3\n",
      "1468 4:53:40 - prob: 0.0\n",
      "1469 4:53:60 - prob: 0.0\n",
      "1470 4:53:80 - prob: 0.0\n",
      "1471 4:54:00 - prob: 0.0\n",
      "1472 4:54:20 - prob: 0.0\n",
      "1473 4:54:40 - prob: 0.0\n",
      "1474 4:54:60 - prob: 0.1\n",
      "1475 4:54:80 - prob: 75.7\n",
      "1476 4:55:00 - prob: 99.9\n",
      "1477 4:55:20 - prob: 0.0\n",
      "1478 4:55:40 - prob: 0.1\n",
      "1479 4:55:60 - prob: 0.0\n",
      "1480 4:55:80 - prob: 0.0\n",
      "1481 4:56:00 - prob: 0.0\n",
      "1482 4:56:20 - prob: 0.0\n",
      "1483 4:56:40 - prob: 0.0\n",
      "1484 4:56:60 - prob: 0.0\n",
      "1485 4:56:80 - prob: 0.0\n",
      "1486 4:57:00 - prob: 0.0\n",
      "1487 4:57:20 - prob: 0.0\n",
      "1488 4:57:40 - prob: 0.0\n",
      "1489 4:57:60 - prob: 0.0\n",
      "1490 4:57:80 - prob: 0.1\n",
      "1491 4:58:00 - prob: 0.0\n",
      "1492 4:58:20 - prob: 0.1\n",
      "1493 4:58:40 - prob: 0.0\n",
      "1494 4:58:60 - prob: 1.6\n",
      "1495 4:58:80 - prob: 1.4\n",
      "1496 4:59:00 - prob: 59.2\n",
      "1497 4:59:20 - prob: 0.1\n",
      "1498 4:59:40 - prob: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nicol\\AppData\\Local\\Temp\\ipykernel_7212\\1571403754.py:10: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  print(i + 1, str(int(i / 5 / 60)) + ':' + seconds, '- prob: ' + str(int(result[i] * 1000) / 10))\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(result)):\n",
    "    seconds = int(i / 5 % 60)\n",
    "    if seconds < 10:\n",
    "        seconds = '0' + str(seconds)\n",
    "    else:\n",
    "        seconds = str(seconds)\n",
    "\n",
    "    seconds += f':{(i%5)*2}0'\n",
    "\n",
    "    #if result[i] > 0.5:\n",
    "    print(i + 1, str(int(i / 5 / 60)) + ':' + seconds, '- prob: ' + str(int(result[i] * 1000) / 10))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T19:50:08.469567400Z",
     "start_time": "2023-09-10T19:50:08.395564600Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 9) Interpretazione dei risultati\n",
    "Questo blocco di codice definisce la funzione per stampare le etichette, ovvero dichiarare l'inizio e la fine di uno o più fischi consecutivi all'interno della registrazione, salvandole in un file di testo.\n",
    "Per il posizionamento delle etichette si fa una precisazione, aggiungere 1 all'indice significa aggiungere 200 ms."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "def makeAndSaveLabels(result, filename):\n",
    "\n",
    "    etichette = []\n",
    "    indexList = []\n",
    "    indexStart = -1\n",
    "    indexStop = -1\n",
    "    hold = 0\n",
    "\n",
    "    for i in range(len(result)):\n",
    "        if not(hold == 0):\n",
    "            if (result[i] > 0.5 and hold==2) or (result[i] > 0.70 and hold==1):\n",
    "                indexList.append(i)\n",
    "            else:\n",
    "                indexStop = i+1.5*(int(i<len(result)-1)) #non sarà mai -1\n",
    "                #scrivi in etichetta    indexStart  indexStop\n",
    "                etichette.append(f'{(indexStart+1)/5}\\t{indexStop/5}\\tW')\n",
    "                hold=0\n",
    "\n",
    "        else:\n",
    "\n",
    "            next = i + 1 - int(i==(len(result)-1))\n",
    "\n",
    "            if result[i] > 0.7:\n",
    "                if(result[abs(i-1)]>0.7 or result[next]>0.7):\n",
    "                    indexList.append(i)\n",
    "                    indexStart = i\n",
    "                    hold = 1\n",
    "\n",
    "                if result[i] > 0.9:\n",
    "                    if result[abs(i-1)]>0.5 or result[next]>0.5 :\n",
    "                        indexList.append(i)\n",
    "                        indexStart = i\n",
    "                        hold = 2\n",
    "\n",
    "                if result[i] > 0.995:\n",
    "                    indexList.append(i)\n",
    "                    indexStart = i\n",
    "                    hold = 2\n",
    "\n",
    "\n",
    "    # Write to the file\n",
    "    with open(filename, 'w') as file:\n",
    "        for item in etichette:\n",
    "            file.write('%s\\n' % item)\n",
    "\n",
    "    return etichette"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T20:23:38.242196200Z",
     "start_time": "2023-09-10T20:23:38.209619100Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "filename = 'Registrazioni_Tesi\\\\_.txt'\n",
    "etichette = makeAndSaveLabels(result, filename)\n",
    "\n",
    "print(len(etichette))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-10T20:00:48.268958Z",
     "start_time": "2023-09-10T20:00:48.245599300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T10:30:00.640010200Z",
     "start_time": "2023-09-09T10:30:00.610009200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-09T08:12:12.057838800Z",
     "start_time": "2023-09-09T08:12:11.930839200Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 10) Processamento audio e stampaggio etichette singolo file\n",
    "In questa parte di codice viene semplicemente definita la funzione da richiamare nel caso in cui si voglia processare un file audio e stamparne le etichette in automatico"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "outputs": [],
   "source": [
    "def predictWaveAndSaveLabels(fileWave, model, outputFile):\n",
    "    pathSpectrograms = 'Analize'\n",
    "    delete_all_files_in_directory(pathSpectrograms)\n",
    "    getAndSaveSpectrogram(fileWave, pathSpectrograms)\n",
    "    result = predictOnSpectrogramsBatches(pathSpectrograms, model, True)\n",
    "    makeAndSaveLabels(result, outputFile)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 11) Processamento audio e stampaggio etichette di tutti i file Wave in una cartella\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pathWaves = 'Registrazioni_Tesi'\n",
    "listWaves = []\n",
    "listWaves = os.listdir(pathWaves)\n",
    "print(listWaves)\n",
    "\n",
    "for recName in listWaves:\n",
    "\n",
    "    if(recName.split('.')[1] != 'wav'):\n",
    "        continue\n",
    "\n",
    "    delete_all_files_in_directory('Analize')\n",
    "    fileWave = os.path.join(pathWaves, recName)\n",
    "    print('\\n\\n\\n\\n', recName)\n",
    "\n",
    "    try:\n",
    "        recName = recName.split('.')[0]\n",
    "        fileLabels = os.path.join(pathWaves, recName) + '.txt'\n",
    "\n",
    "        predictWaveAndSaveLabels(fileWave, model, fileLabels)\n",
    "\n",
    "        #gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "            print(f\"Error! Reason: {e}\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-08T08:23:03.513508100Z",
     "start_time": "2023-09-08T08:23:03.468185900Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
